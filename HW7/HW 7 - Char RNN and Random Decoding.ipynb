{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97ab5757",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alex\\miniconda3\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ffd0ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILL_IN = \"FILL_IN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd215dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64c0320b",
   "metadata": {},
   "source": [
    "### Get the data and process\n",
    "- This is the Mysterious island found in Project Gutenberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bb6aa0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "766"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('1268-0.txt', 'r', encoding=\"utf8\") as fp:\n",
    "    text=fp.read()\n",
    "\n",
    "text.index('THE MYSTERIOUS ISLAND')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4e64a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Reading and processing text\n",
    "# with open('1268-0.txt', 'r', encoding=\"utf8\") as fp:\n",
    "#     text=fp.read()\n",
    "\n",
    "# # Get the index of 'THE MYSTERIOUS ISLAND'\n",
    "# start_indx = text.index('THE MYSTERIOUS ISLAND')\n",
    "# # Get the index of 'End of the Project Gutenberg'\n",
    "# end_indx = text.index('END OF THE PROJECT GUTENBERG')\n",
    "\n",
    "# # Set text to the text between start and end idx.\n",
    "# text = text[start_indx:end_indx]\n",
    "# # Get the unique set of characters.\n",
    "# char_set = set(text)\n",
    "# print('Total Length:', len(text))\n",
    "# print('Unique Characters:', len(char_set))\n",
    "# assert(len(text) == 1130711)\n",
    "# assert(len(char_set) == 85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5067480c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Length: 1112300\n",
      "Unique Characters: 80\n"
     ]
    }
   ],
   "source": [
    "## Reading and processing text\n",
    "with open('1268-0.txt', 'r', encoding=\"utf8\") as fp:\n",
    "    text=fp.read()\n",
    "\n",
    "# Get the index of 'THE MYSTERIOUS ISLAND'\n",
    "start_indx = text.index('THE MYSTERIOUS ISLAND')\n",
    "# Get the index of 'End of the Project Gutenberg'\n",
    "end_indx = text.index('END OF THE PROJECT GUTENBERG')\n",
    "\n",
    "# Set text to the text between start and end idx.\n",
    "text = text[start_indx:end_indx]\n",
    "# Get the unique set of characters.\n",
    "char_set = set(text)\n",
    "print('Total Length:', len(text))\n",
    "print('Unique Characters:', len(char_set))\n",
    "assert(len(text) == 1112300)\n",
    "assert(len(char_set) == 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76393bdb",
   "metadata": {},
   "source": [
    "### Tokenze and get other helpers\n",
    "- We do this manually since everything is character based."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a445114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text encoded shape:  (1112300,)\n",
      "THE MYSTERIOUS       == Encoding ==>  [44 32 29  1 37 48 43 44 29 42 33 39 45 43  1]\n",
      "[33 43 36 25 38 28]  == Reverse  ==>  ISLAND\n"
     ]
    }
   ],
   "source": [
    "# The universe of words.\n",
    "chars_sorted = sorted(char_set)\n",
    "\n",
    "# Effectively, these maps are the tokenizer.\n",
    "# Map each char to a unique int. This is a dict.\n",
    "char2int = {c:i for i, c in enumerate(chars_sorted)}\n",
    "# Do the revverse of the above, this should be a np array.\n",
    "int2char = np.array(chars_sorted)\n",
    "\n",
    "# Tokenize the entire corpus. This should be an np array of np.int32 type.\n",
    "text_encoded = np.array([char2int[char] for char in text], dtype=np.int32)\n",
    "\n",
    "print('Text encoded shape: ', text_encoded.shape)\n",
    "\n",
    "print(text[:15], '     == Encoding ==> ', text_encoded[:15])\n",
    "print(text_encoded[15:21], ' == Reverse  ==> ', ''.join(int2char[text_encoded[15:21]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8e0270",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "720cd752",
   "metadata": {},
   "source": [
    "#### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2743a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text encoded shape:  (1112300,)\n",
      "THE MYSTERIOUS       == Encoding ==>  [44 32 29  1 37 48 43 44 29 42 33 39 45 43  1]\n",
      "[33 43 36 25 38 28]  == Reverse  ==>  ISLAND\n"
     ]
    }
   ],
   "source": [
    "print('Text encoded shape: ', text_encoded.shape)\n",
    "print(text[:15], '     == Encoding ==> ', text_encoded[:15])\n",
    "print(text_encoded[15:21], ' == Reverse  ==> ', ''.join(int2char[text_encoded[15:21]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "367e733d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert(\n",
    "#     np.array_equal(\n",
    "#     text_encoded[:15],\n",
    "#         [48, 36, 33, 1, 41, 53, 47, 48, 33, 46, 37, 43, 49, 47,  1]\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cdcafe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(\n",
    "    np.array_equal(\n",
    "    text_encoded[:15],\n",
    "        [44, 32, 29,  1, 37, 48, 43, 44, 29, 42, 33, 39, 45, 43,  1]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c418ca0",
   "metadata": {},
   "source": [
    "### Process the data and get the data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f429dc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 40\n",
    "chunk_size = seq_length + 1\n",
    "\n",
    "# Break up the data into chunks of size 41. This should be a list of lists.\n",
    "# Use text_encoded. This will be used to get (x, y) pairs.\n",
    "text_chunks = [text_encoded[i:i+chunk_size]for i in range(0, len(text_encoded)-chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e329fffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\AppData\\Local\\Temp\\ipykernel_37040\\788824669.py:15: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:233.)\n",
      "  seq_dataset = TextDataset(torch.tensor(text_chunks))\n"
     ]
    }
   ],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text_chunks):\n",
    "        self.text_chunks = text_chunks\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_chunks)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get the text chunk at index idx.\n",
    "        text_chunk = text_chunks[idx]\n",
    "        # Return (x, y) where x has length 40 and y has length 40.\n",
    "        # y should be x shifted by 1 time.\n",
    "        return torch.tensor(text_chunk[:-1]), torch.tensor(text_chunk[1:])\n",
    "    \n",
    "seq_dataset = TextDataset(torch.tensor(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71328555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40]) torch.Size([40])\n",
      "Input (x): 'THE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nTHE MYSTER'\n",
      "Target (y): 'HE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nTHE MYSTERI'\n",
      "\n",
      "torch.Size([40]) torch.Size([40])\n",
      "Input (x): 'HE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nTHE MYSTERI'\n",
      "Target (y): 'E MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nTHE MYSTERIO'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, (seq, target) in enumerate(seq_dataset):\n",
    "    # 40 characters for source and target ...\n",
    "    print(seq.shape, target.shape)\n",
    "    print('Input (x):', repr(''.join(int2char[seq])))\n",
    "    print('Target (y):', repr(''.join(int2char[target])))\n",
    "    print()\n",
    "    if i == 1:\n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebb989c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a881b316",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "torch.manual_seed(1)\n",
    "seq_dl = DataLoader(seq_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f77f7f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45ed0b2f",
   "metadata": {},
   "source": [
    "### Write the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b4cbf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size):\n",
    "        super().__init__()\n",
    "        # Set to an embedding layer of vocab_size by embed_dim.\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embed_dim) \n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "        # Set to an LSTM with x having embed_dim and h dimension rnn_hidden_size.\n",
    "        # batch_first shoould be true.\n",
    "        self.rnn = torch.nn.LSTM(input_size=embed_dim, \n",
    "                                 hidden_size=self.rnn_hidden_size, \n",
    "                                 batch_first=True)\n",
    "        \n",
    "        # Make a linear layer from rnn_hidden_size to vocab_size.\n",
    "        # This will be used to get the yt for each xt.\n",
    "        self.fc = torch.nn.Linear(rnn_hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, text, hidden=None, cell=None):\n",
    "        # Get the embeddings for text.\n",
    "        out = self.embedding(text)\n",
    "        \n",
    "        # Pass out, hidden and cell through the rnn.\n",
    "        # If hidden is None, don't specify it and just use out.\n",
    "        if hidden is not None:\n",
    "            out, (hidden, cell) = self.rnn(out,(hidden, cell))\n",
    "        else:\n",
    "            out, (hidden, cell) = self.rnn(out)\n",
    "        \n",
    "        # Pass out through fc.\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out, (hidden, cell)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Initialize to zeros of 1 by ??? appropriate dimensions.\n",
    "        hidden = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
    "        cell = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
    "        return hidden.to(device), cell.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16c03dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00789dfd",
   "metadata": {},
   "source": [
    "### Do this right way - across all data all at once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33380607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(80, 256)\n",
       "  (rnn): LSTM(256, 512, batch_first=True)\n",
       "  (fc): Linear(in_features=512, out_features=80, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(int2char)\n",
    "embed_dim = 256\n",
    "rnn_hidden_size = 512\n",
    "\n",
    "torch.manual_seed(1)\n",
    "model = RNN(vocab_size, embed_dim, rnn_hidden_size) \n",
    "model = model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f47f48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 3.5844\n",
      "Epoch 100 loss: 1.6981\n",
      "Epoch 200 loss: 1.5276\n",
      "Epoch 300 loss: 1.4950\n",
      "Epoch 400 loss: 1.3867\n",
      "Epoch 500 loss: 1.4001\n",
      "Epoch 600 loss: 1.3485\n",
      "Epoch 700 loss: 1.2865\n",
      "Epoch 800 loss: 1.3097\n",
      "Epoch 900 loss: 1.3092\n",
      "Epoch 1000 loss: 1.3148\n",
      "Epoch 1100 loss: 1.2799\n",
      "Epoch 1200 loss: 1.3213\n",
      "Epoch 1300 loss: 1.2625\n",
      "Epoch 1400 loss: 1.2436\n",
      "Epoch 1500 loss: 1.2614\n",
      "Epoch 1600 loss: 1.2811\n",
      "Epoch 1700 loss: 1.2191\n",
      "Epoch 1800 loss: 1.1974\n",
      "Epoch 1900 loss: 1.2199\n",
      "Epoch 2000 loss: 1.2419\n",
      "Epoch 2100 loss: 1.2105\n",
      "Epoch 2200 loss: 1.1819\n",
      "Epoch 2300 loss: 1.1465\n",
      "Epoch 2400 loss: 1.1910\n",
      "Epoch 2500 loss: 1.1993\n",
      "Epoch 2600 loss: 1.1632\n",
      "Epoch 2700 loss: 1.1891\n",
      "Epoch 2800 loss: 1.1852\n",
      "Epoch 2900 loss: 1.1843\n",
      "Epoch 3000 loss: 1.2206\n",
      "Epoch 3100 loss: 1.2032\n",
      "Epoch 3200 loss: 1.1711\n",
      "Epoch 3300 loss: 1.1830\n",
      "Epoch 3400 loss: 1.2539\n",
      "Epoch 3500 loss: 1.1283\n",
      "Epoch 3600 loss: 1.1700\n",
      "Epoch 3700 loss: 1.1328\n",
      "Epoch 3800 loss: 1.2670\n",
      "Epoch 3900 loss: 1.1779\n",
      "Epoch 4000 loss: 1.1741\n",
      "Epoch 4100 loss: 1.1548\n",
      "Epoch 4200 loss: 1.1904\n",
      "Epoch 4300 loss: 1.1653\n",
      "Epoch 4400 loss: 1.1184\n",
      "Epoch 4500 loss: 1.1785\n",
      "Epoch 4600 loss: 1.1154\n",
      "Epoch 4700 loss: 1.1511\n",
      "Epoch 4800 loss: 1.1074\n",
      "Epoch 4900 loss: 1.1376\n",
      "Epoch 5000 loss: 1.1445\n",
      "Epoch 5100 loss: 1.1792\n",
      "Epoch 5200 loss: 1.1956\n",
      "Epoch 5300 loss: 1.1541\n",
      "Epoch 5400 loss: 1.1499\n",
      "Epoch 5500 loss: 1.1768\n",
      "Epoch 5600 loss: 1.1263\n",
      "Epoch 5700 loss: 1.0408\n",
      "Epoch 5800 loss: 1.1519\n",
      "Epoch 5900 loss: 1.1700\n",
      "Epoch 6000 loss: 1.1323\n",
      "Epoch 6100 loss: 1.1698\n",
      "Epoch 6200 loss: 1.1464\n",
      "Epoch 6300 loss: 1.1927\n",
      "Epoch 6400 loss: 1.1400\n",
      "Epoch 6500 loss: 1.1293\n",
      "Epoch 6600 loss: 1.1793\n",
      "Epoch 6700 loss: 1.1357\n",
      "Epoch 6800 loss: 1.1360\n",
      "Epoch 6900 loss: 1.1159\n",
      "Epoch 7000 loss: 1.1395\n",
      "Epoch 7100 loss: 1.2006\n",
      "Epoch 7200 loss: 1.1732\n",
      "Epoch 7300 loss: 1.1279\n",
      "Epoch 7400 loss: 1.1807\n",
      "Epoch 7500 loss: 1.1471\n",
      "Epoch 7600 loss: 1.1378\n",
      "Epoch 7700 loss: 1.1503\n",
      "Epoch 7800 loss: 1.1336\n",
      "Epoch 7900 loss: 1.1466\n",
      "Epoch 8000 loss: 1.1638\n",
      "Epoch 8100 loss: 1.1602\n",
      "Epoch 8200 loss: 1.1639\n",
      "Epoch 8300 loss: 1.0791\n",
      "Epoch 8400 loss: 1.0956\n",
      "Epoch 8500 loss: 1.1594\n",
      "Epoch 8600 loss: 1.1388\n",
      "Epoch 8700 loss: 1.1500\n",
      "Epoch 8800 loss: 1.1653\n",
      "Epoch 8900 loss: 1.1669\n",
      "Epoch 9000 loss: 1.1566\n",
      "Epoch 9100 loss: 1.1313\n",
      "Epoch 9200 loss: 1.1432\n",
      "Epoch 9300 loss: 1.1699\n",
      "Epoch 9400 loss: 1.0864\n",
      "Epoch 9500 loss: 1.1465\n",
      "Epoch 9600 loss: 1.1429\n",
      "Epoch 9700 loss: 1.1696\n",
      "Epoch 9800 loss: 1.1534\n",
      "Epoch 9900 loss: 1.1165\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "# Set to 10000.\n",
    "num_epochs = 10000\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# epochs here will mean batches.\n",
    "# If the above takes too long, use 1000.\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    hidden, cell = model.init_hidden(batch_size)\n",
    "    \n",
    "    # Get the next batch from seq_dl\n",
    "    seq_batch, target_batch = next(iter(seq_dl))\n",
    "    seq_batch, target_batch = seq_batch.type(torch.LongTensor), target_batch.type(torch.LongTensor)\n",
    "        \n",
    "    seq_batch = seq_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss = 0\n",
    "\n",
    "    # Pass through the model.\n",
    "    logits, _ = model(seq_batch, hidden, cell)\n",
    "    \n",
    "    # Get the loss.\n",
    "    # You'll need to reshape / view things to make this work.\n",
    "    loss += criterion(logits.view(-1,vocab_size), target_batch.view(-1))\n",
    "        \n",
    "    # Do back prop.\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Get the value in the tensor loss.\n",
    "    loss = loss.item()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch} loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "20979e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities: tensor([[0.0159, 0.1173, 0.8668]])\n",
      "[[1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]]\n"
     ]
    }
   ],
   "source": [
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "logits = torch.tensor([[-1.0, 1.0, 3.0]])\n",
    "\n",
    "# Get the probabilities for these logits.\n",
    "print('Probabilities:', torch.nn.Softmax(dim=1)(logits))\n",
    "\n",
    "# Get a Categorical random variable with the above probabilities for each of the classes.\n",
    "m = Categorical(probs=torch.nn.Softmax(dim=1)(logits))\n",
    "# Generate 10 things.\n",
    "samples = m.sample(torch.Size([10]))\n",
    "\n",
    "print(samples.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "81ec176d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m logits \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([[\u001b[39m-\u001b[39m\u001b[39m1.0\u001b[39m, \u001b[39m1.0\u001b[39m, \u001b[39m3.0\u001b[39m]])\n\u001b[1;32m----> 2\u001b[0m m \u001b[39m=\u001b[39m Categorical(logits\u001b[39m.\u001b[39;49mnumpy())\n",
      "File \u001b[1;32mc:\\Users\\Alex\\miniconda3\\lib\\site-packages\\torch\\distributions\\categorical.py:55\u001b[0m, in \u001b[0;36mCategorical.__init__\u001b[1;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mEither `probs` or `logits` must be specified, but not both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     54\u001b[0m \u001b[39mif\u001b[39;00m probs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 55\u001b[0m     \u001b[39mif\u001b[39;00m probs\u001b[39m.\u001b[39;49mdim() \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m     56\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`probs` parameter must be at least one-dimensional.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     57\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprobs \u001b[39m=\u001b[39m probs \u001b[39m/\u001b[39m probs\u001b[39m.\u001b[39msum(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'dim'"
     ]
    }
   ],
   "source": [
    "logits = torch.tensor([[-1.0, 1.0, 3.0]])\n",
    "m = Categorical(logits.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0547467d",
   "metadata": {},
   "source": [
    "### Random decoding.\n",
    "- This compounds problems: once you make a mistake, you can't undo it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f68537dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([44, 57, 54,  1, 58, 68, 61, 50, 63, 53])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input = torch.tensor([char2int[s] for s in 'The island'])\n",
    "encoded_input.view(1,-1).shape\n",
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa9cdf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "614fb236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 512]) torch.Size([1, 1, 512])\n",
      "The island?”\n",
      "\n",
      "“Forward!” exclaimed Harding.\n",
      "\n",
      "Neb heard, saying to them. The road of their approaches, and in a stormy half an idea, but he had obscuceful\n",
      "with and did not made to a hundred feet into a\n",
      "human been quickly by pyrite which the convicts hixed more than the long, questions, he tried, and there wake their situations when all this master!”\n",
      "\n",
      "“Or the dockyard known about their own banks, there was not moving it quitted,” added the reporter, “she at the lower mass of asked.\n",
      "\n",
      "At a smoke, so devoted t\n"
     ]
    }
   ],
   "source": [
    "def random_sample(\n",
    "    model,\n",
    "    starting_str, \n",
    "    len_generated_text=500, \n",
    "):\n",
    "\n",
    "    # Encode starting string into a tensor using char2str.\n",
    "    encoded_input = torch.tensor([char2int[s] for s in starting_str]).to(device)\n",
    "    \n",
    "    # Reshape to be 1 by ??? - let PyTorch figure this out.\n",
    "    encoded_input = encoded_input.view(1,-1)\n",
    "\n",
    "    # This will be what you generate, but it starts off with something.\n",
    "    generated_str = starting_str\n",
    "\n",
    "    # Put model in eval mode. This matters if we had dropout o batch / layer norms.\n",
    "    model.eval()\n",
    "    \n",
    "    hidden, cell = model.init_hidden(1)\n",
    "    print(hidden.shape, cell.shape)\n",
    "    \n",
    "    hidden = hidden.to(device)\n",
    "    \n",
    "    cell = cell.to(device)\n",
    "        \n",
    "    # Build up the starting hidden and cell states.\n",
    "    # You can do this all in one go?\n",
    "    for c in range(len(starting_str)-1):\n",
    "        # Feed each letter 1 by 1 and then get the final hidden state.\n",
    "        out = encoded_input[:,c].unsqueeze(0)\n",
    "        # Pass out through, note we update hidden and cell and use them again\n",
    "        _, (hidden, cell) = model(out, hidden, cell)\n",
    "    \n",
    "    # Gte the last char; note we did not do go to the last char above.\n",
    "    last_char = encoded_input[:,-1]\n",
    "    # Generate chars one at a time, add them to generated_str.\n",
    "    # Do this over and over until you get the desired length.\n",
    "\n",
    "    for i in range(len_generated_text):\n",
    "        last_char = last_char.unsqueeze(0)\n",
    "        # Use hidden and cell from the above.\n",
    "        # Use last_char, which will be updated over and over.\n",
    "        logits, (hidden, cell) = model(last_char, hidden, cell)\n",
    "        # Get the logits.\n",
    "        logits = logits.squeeze()\n",
    "        \n",
    "        # m is a random variable with probabilities based on the softmax of the logits.\n",
    "        m = Categorical(probs=torch.nn.Softmax(dim=0)(logits))\n",
    "        \n",
    "        # Generate from m 1 char.\n",
    "        last_char = m.sample().unsqueeze(0)\n",
    "        \n",
    "        # Add the geenrated char to generated_str, but pass it through int2str so that\n",
    "        generated_str += int2char[last_char.item()]\n",
    "        \n",
    "    return generated_str\n",
    "\n",
    "torch.manual_seed(1)\n",
    "model.to(device)\n",
    "print(random_sample(model, starting_str='The island'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f58492",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

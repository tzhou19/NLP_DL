{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5db00378",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexzhou/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f79343",
   "metadata": {},
   "source": [
    "### Information\n",
    "- We will do a few preliminary exercises and also build a character level MLP language model.\n",
    "- This model will be similar to the model we did in class, except that we will have characters as tokens, not words.\n",
    "- You will need a conda environment for this, here is general information on this.\n",
    " - https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html\n",
    " - PyTorch: https://anaconda.org/pytorch/pytorch\n",
    " \n",
    "In the code below, FILL-IN the code necessary in the hint string provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb95b07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81cb873f",
   "metadata": {},
   "source": [
    "### Preliminary exercises\n",
    "- Please fill in the cells below with the asked for data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a23f069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x111334150>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbf70a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = nn.Embedding(10,5)\n",
    "v = e(torch.tensor(3))\n",
    "l = e.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "568fe020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.8246, -0.8029, -0.8771,  0.9178, -1.1951],\n",
       "        [-0.7006,  0.8931,  0.5070,  0.5189,  0.8453],\n",
       "        [ 0.9370, -0.5061,  0.7507, -0.0704,  0.3276],\n",
       "        [-0.3582,  0.6241, -0.0508, -1.0284, -0.6014],\n",
       "        [ 0.5474,  0.1593,  0.6124,  0.1262,  0.9572],\n",
       "        [ 0.0257,  1.0389, -2.3217,  0.1992,  0.4714],\n",
       "        [ 0.1319, -1.6183, -0.2496,  1.1969,  1.4502],\n",
       "        [ 1.3331, -0.1006, -2.0760,  1.1813, -0.6348],\n",
       "        [ 0.1365, -2.6525, -0.0141, -2.1362,  0.2617],\n",
       "        [-0.1473,  0.2094, -0.7971,  0.2595,  0.7638]], requires_grad=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3683b288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=10, out_features=5, bias=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f91a6ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an embedding layer for a vocabulary of size 10 and the word vectors are each of dimension 5.\n",
    "e = nn.Embedding(10,5)\n",
    "\n",
    "# Extract the embedding for the word whose token index is 3. What is the shape of this vector?\n",
    "v = e(torch.tensor(3))\n",
    "\n",
    "# Extract the weight matrix from the layer e.\n",
    "# Create a linear layer (with no bias) of size 10 by 5 and set it's data to the embedding matrix.\n",
    "l = nn.Linear(5,10, bias = False)\n",
    "l.weight = e.weight\n",
    "\n",
    "# Insert inside of the assert below some sort of equality check between l.weight and e.weight; it should pass to true.\n",
    "# Hint: look up torch.all() and torch.eq()\n",
    "assert(torch.eq(e.weight, l.weight).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e37fec18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a batch of size 2 with entries [0, 1, 2] and [2, 3, 4] in the data batch.\n",
    "x = torch.tensor([[0, 1, 2], [2, 3, 4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d6f3a839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the dimesion of this batch ran through the embeding layer?\n",
    "assert(e(x).shape == torch.Size([2,3,5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3422e57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9140e3c",
   "metadata": {},
   "source": [
    "### Constants and configs used below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c9ace94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cpu\"\n",
    "LR = 4.0\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 5\n",
    "MARKER = '.'\n",
    "# N-gram level; P(w_t | w_{t-1}, ..., w_{t-n+1}).\n",
    "# We use 3 words to predict the next word.\n",
    "n = 4\n",
    "# Hidden layer dimension.\n",
    "h = 20\n",
    "# Word embedding dimension.\n",
    "m = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff816ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09f16039",
   "metadata": {},
   "source": [
    "### Get the dataset and the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9c4dca3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDataset(Dataset):\n",
    "    def __init__(self, words, chars):\n",
    "        self.words = words\n",
    "        self.chars = chars\n",
    "        # Inverse dictionaries mapping char tokens to unique ids and the reverse.\n",
    "        # Tokens in this case are the unique chars we passed in above.\n",
    "        # Each token should be mappend to a unique integer and MARKER should have token 0.\n",
    "        # For example, stoi should be like {'.' -> 0, 'a' -> 1, 'b' -> 2} if I pass in chars = '.ab'.\n",
    "        dic_stoi, dic_itos = {}, {}\n",
    "        for ele in chars:\n",
    "            count = 0\n",
    "            dic_stoi[ele] = count\n",
    "            dic_itos[count] = ele\n",
    "            count += 1\n",
    "        self.stoi = dic_stoi\n",
    "        self.itos = dic_itos # Inverse mapping.\n",
    "\n",
    "    def __len__(self):\n",
    "        # Number of words.\n",
    "        return len(self.words)\n",
    "\n",
    "    def contains(self, word):\n",
    "        # Check if word is in self.words and return True/False if it is, is not.\n",
    "        return True if word in self.words else False\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        # Return the vocabulary size.\n",
    "        return len(self.chars)\n",
    "\n",
    "    def encode(self, word):\n",
    "        # Express this word as a list of int ids. For example, maybe \".abc\" -> [0, 1, 2, 3].\n",
    "        # This assumes 'a' -> 1, etc.\n",
    "        result = []\n",
    "        for char in word:\n",
    "            result.append(self.stoi[char])\n",
    "        return result\n",
    "    \n",
    "    def decode(self, tokens):\n",
    "        # For a set of tokens, return back the string.\n",
    "        # For example, maybe [1, 1, 2] -> \"aac\"\n",
    "        result = []\n",
    "        for tok in tokens:\n",
    "            result.append(self.dic_itos[tok])\n",
    "        return result\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # This is used so we can loop over the data.\n",
    "        word = self.words[idx]\n",
    "        return self.encode(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "21bc4e92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'......'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'..'*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "32e480ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(window, input_file = 'names.txt'):\n",
    "    \"\"\"\n",
    "    This takes a file of words and separates all the words.\n",
    "    It then gets all the characters present in the universe of words and then ouputs the statistics. \n",
    "    \"\"\"\n",
    "    with open(input_file, 'r') as f:\n",
    "        data = f.read()\n",
    "    # Split the file by new lines. You should get a list of names.\n",
    "    words = data.split('\\n')\n",
    "    words = [word.replace(' ', '') for word in words] # This gets rid of any trailing and starting white spaces.\n",
    "    words = [word for word in words if word] # Filter out all the empty words.\n",
    "\n",
    "    \n",
    "    chars = sorted(list(set([char for word in words for char in word]))) # This gets the universe of all characters.\n",
    "    \n",
    "    # Will force chars to have MARKER having index 0.\n",
    "    chars= [MARKER] + chars\n",
    "    \n",
    "    # Pad each word with a context window of size n-1.\n",
    "    # Why? a word like \"abc\" should becomes \"..abc..\" if the window is size 3.\n",
    "    # This is some we can get pair of (x, y) data like this: \"..\" -> \"a\", \".a\" -> \"b\", \"ab\" -> \"c\", \"bc\" -> \".\", \"c.\" -> \".\"\n",
    "    # I.e. this allows us to know that \"a\" is a start character.\n",
    "    # So you should get something like [\"ab\", \"c\"] -> [\"..ab..\", \"..c..\"], for example.\n",
    "    words = [('.'*(window-1))+word+('.'*(window-1)) for word in words]\n",
    "            \n",
    "    print(f\"The number of examples in the dataset: {len(words)}\")\n",
    "    print(f\"The number of unique characters in the vocabulary: {len(chars)}\")\n",
    "    print(f\"The vocabulary we have is: {''.join(chars)}\")\n",
    "\n",
    "    # Partition the input data into a training, validation, and the test set.\n",
    "    out_of_sample_set_size = min(2000, int(len(words) * 0.1)) # We use 10% of the training set, or up to 2000 examples.\n",
    "    test_set_size = 1500\n",
    "    \n",
    "    # First, get a random permutation of randomly permute of size len(words).\n",
    "    # Then, convert this to a list. \n",
    "    # This index list is used below to get the train, validation, and test sets.\n",
    "    rp = torch.randperm(len(words)).tolist()\n",
    "    \n",
    "    # Get train, validation, and test set.\n",
    "    train_words = [words[i] for i in rp[:-out_of_sample_set_size]]\n",
    "    validation_words = [words[i] for i in rp[-out_of_sample_set_size:-test_set_size]]\n",
    "    test_words = [words[i] for i in rp[-test_set_size:]]    \n",
    "    \n",
    "    print(f\"We've split up the dataset into {len(train_words)}, {len(validation_words)}, {len(test_words)} training, validation, and test examples\")\n",
    "\n",
    "    # But the data in the data set objects.\n",
    "    train_dataset = CharDataset(train_words, chars)\n",
    "    validation_dataset = CharDataset(validation_words, chars)\n",
    "    test_dataset = CharDataset(test_words, chars)\n",
    "\n",
    "    return train_dataset, validation_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "36b97647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of examples in the dataset: 32033\n",
      "The number of unique characters in the vocabulary: 27\n",
      "The vocabulary we have is: .abcdefghijklmnopqrstuvwxyz\n",
      "We've split up the dataset into 30033, 500, 1500 training, validation, and test examples\n"
     ]
    }
   ],
   "source": [
    "train_dataset, validation_dataset, test_dataset = create_datasets(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707b45bc",
   "metadata": {},
   "source": [
    "## Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c6cb698c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'...eimy...'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the first word in \"train_dataset\"\n",
    "train_dataset.words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8369b489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# Get the stoi map of train_dataset. How many keys does it have?\n",
    "print(len(train_dataset.stoi))\n",
    "print(train_dataset.get_vocab_size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9c5ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e4881fe",
   "metadata": {},
   "source": [
    "### Get the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8f7967c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(dataset, window):\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "    # For ech word.\n",
    "    for i, word in enumerate(dataset):\n",
    "        # Grab a context of size window and window-1 characters will be in x, 1 will be in y.\n",
    "        for j, _ in enumerate(word):\n",
    "            # If there is no widow of size window left, break.\n",
    "            if j + window > len(word) - 1:\n",
    "                break\n",
    "            word_window = word[j:j+window]\n",
    "            x, y = word_window[:window-1], word_window[-1]\n",
    "            x_list.append(x)\n",
    "            y_list.append(y)\n",
    "            \n",
    "    return DataLoader(\n",
    "        TensorDataset(torch.tensor(x_list), torch.tensor(y_list)),\n",
    "        BATCH_SIZE,\n",
    "        shuffle=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5086d90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = create_dataloader(train_dataset, n)\n",
    "validation_dataloader = create_dataloader(validation_dataset, n)\n",
    "test_dataloader = create_dataloader(test_dataset, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c8d054",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eca51b36",
   "metadata": {},
   "source": [
    "### Set up the model\n",
    "- Identical to lecture. Please look over that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8ab8cb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One of the first Neural language models!\n",
    "class CharacterNeuralLanguageModel(nn.Module):\n",
    "    def __init__(self, V, m, h, n):\n",
    "        super(CharacterNeuralLanguageModel, self).__init__()\n",
    "        \n",
    "        # Vocabulary size.\n",
    "        self.V = \" FILL_IN \"\n",
    "        \n",
    "        # Embedding dimension, per word.\n",
    "        self.m = \" FILL_IN \"\n",
    "        \n",
    "        # Hidden dimension.\n",
    "        self.h = \" FILL_IN \"\n",
    "        \n",
    "        # N in \"N-gram\"\n",
    "        self.n = \" FILL_IN \"\n",
    "        \n",
    "        # Can you change all this stuff to use nn.Linear?\n",
    "        # Ca also use nn.Parameter(torch.zeros(V, m)) for self.C but then we need one-hot and this is slow.\n",
    "        self.C = \" FILL_IN \" \n",
    "        self.H = \" FILL_IN \"\n",
    "        self.W = \" FILL_IN \"\n",
    "        self.U = \" FILL_IN \"\n",
    "        \n",
    "        self.b = \" FILL_IN \")\n",
    "        self.d = \" FILL_IN \"\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        # Intitialize C, H, W, U in a nice way. Use xavier initialization for the weights.\n",
    "        # On a first run, just pass.\n",
    "        \" FILL_IN \"\n",
    "        pass # Replace this pass with something else.\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # x is of dimenson N = batch size X n-1\n",
    "        \n",
    "        # N X (n-1) X m \n",
    "        x = \" FILL_IN \"\n",
    "        \n",
    "        # N\n",
    "        N = \" FILL_IN \"\n",
    "        \n",
    "        # N X (n-1) * m\n",
    "        x = \" FILL_IN \"\n",
    "    \n",
    "        # N X V\n",
    "        y = self.b + torch.matmul(x, self.W) + torch.matmul(nn.Tanh()(self.d + torch.matmul(x, self.H)), self.U)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43d569e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "899045e6",
   "metadata": {},
   "source": [
    "### Set up the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "02b8e0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identical to lecture.\n",
    "criterion = \" FILL_IN \"\n",
    "model = CharacterNeuralLanguageModel(\n",
    "    train_dataset.get_vocab_size(), m, h, n\n",
    ").to(DEVICE)\n",
    "optimizer = \" FILL_IN \"\n",
    "scheduler = \" FILL_IN \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10508c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many parameters does the neural network have?\n",
    "# Hint: look up model.named_parameters and the method \"nelement\" on a tensor.\n",
    "# See also the XOR notebook where we count the gradients that are 0.\n",
    "# There, we loop over the parameters.\n",
    "number_parameters = \" FILL_IN \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d91dc71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72b5bb91",
   "metadata": {},
   "source": [
    "### Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e49ebc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(total_loss, total_batches):\n",
    "    return \" FILL_IN \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d58cc1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    total_loss, total_batches = 0.0, 0.0\n",
    "    log_interval = 500\n",
    "\n",
    "    for idx, (x, y) in tqdm(enumerate(dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits = model(x)\n",
    "                        \n",
    "        # Get the loss.\n",
    "        loss = \" FILL_IN \"\n",
    "\n",
    "        # Do back propagation.\n",
    "        loss.backward()\n",
    "                        \n",
    "        # Clip the gradients so they don't explode. Look at how this is done in lecture.\n",
    "        \" FILL_IN \"\n",
    "        \n",
    "        # Do an optimization step.\n",
    "        \" FILL_IN \"\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_batches += 1\n",
    "                \n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            perplexity = calculate_perplexity(total_loss,  total_batches)\n",
    "            print(\n",
    "                \"| epoch {:3d} \"\n",
    "                \"| {:5d}/{:5d} batches \"\n",
    "                \"| perplexity {:8.3f} \"\n",
    "                \"| loss {:8.3f} \"\n",
    "                .format(\n",
    "                    epoch,\n",
    "                    idx,\n",
    "                    len(dataloader),\n",
    "                    perplexity,\n",
    "                    total_loss / total_batches,\n",
    "                )\n",
    "            )\n",
    "            total_loss, total_batches = 0.0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "85722617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader, model, criterion):\n",
    "    model.eval()\n",
    "    total_loss, total_batches = 0.0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (x, y) in enumerate(dataloader):\n",
    "            logits = model(x)\n",
    "            total_loss += criterion(input=logits, target=y.squeeze(-1)).item()\n",
    "            total_batches += 1\n",
    "    return total_loss / total_batches, calculate_perplexity(total_loss,  total_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "21ba24f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1020it [00:00, 2664.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   500/15255 batches | perplexity   10.395 | loss    2.341 \n",
      "| epoch   1 |  1000/15255 batches | perplexity    9.218 | loss    2.221 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1863it [00:00, 2766.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |  1500/15255 batches | perplexity    8.665 | loss    2.159 \n",
      "| epoch   1 |  2000/15255 batches | perplexity    8.565 | loss    2.148 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2995it [00:01, 2814.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |  2500/15255 batches | perplexity    8.649 | loss    2.157 \n",
      "| epoch   1 |  3000/15255 batches | perplexity    8.368 | loss    2.124 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3847it [00:01, 2832.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |  3500/15255 batches | perplexity    8.517 | loss    2.142 \n",
      "| epoch   1 |  4000/15255 batches | perplexity    8.269 | loss    2.112 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4989it [00:01, 2848.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |  4500/15255 batches | perplexity    8.159 | loss    2.099 \n",
      "| epoch   1 |  5000/15255 batches | perplexity    8.112 | loss    2.093 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5843it [00:02, 2837.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |  5500/15255 batches | perplexity    8.444 | loss    2.133 \n",
      "| epoch   1 |  6000/15255 batches | perplexity    8.106 | loss    2.093 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6980it [00:02, 2815.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |  6500/15255 batches | perplexity    8.148 | loss    2.098 \n",
      "| epoch   1 |  7000/15255 batches | perplexity    8.361 | loss    2.124 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7825it [00:02, 2801.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |  7500/15255 batches | perplexity    8.250 | loss    2.110 \n",
      "| epoch   1 |  8000/15255 batches | perplexity    8.428 | loss    2.132 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8955it [00:03, 2817.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |  8500/15255 batches | perplexity    8.339 | loss    2.121 \n",
      "| epoch   1 |  9000/15255 batches | perplexity    8.346 | loss    2.122 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9807it [00:03, 2826.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |  9500/15255 batches | perplexity    8.264 | loss    2.112 \n",
      "| epoch   1 | 10000/15255 batches | perplexity    8.040 | loss    2.084 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10955it [00:03, 2849.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 | 10500/15255 batches | perplexity    7.991 | loss    2.078 \n",
      "| epoch   1 | 11000/15255 batches | perplexity    8.127 | loss    2.095 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11811it [00:04, 2836.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 | 11500/15255 batches | perplexity    8.098 | loss    2.092 \n",
      "| epoch   1 | 12000/15255 batches | perplexity    8.351 | loss    2.122 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12943it [00:04, 2773.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 | 12500/15255 batches | perplexity    8.200 | loss    2.104 \n",
      "| epoch   1 | 13000/15255 batches | perplexity    8.236 | loss    2.109 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13778it [00:04, 2755.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 | 13500/15255 batches | perplexity    8.186 | loss    2.102 \n",
      "| epoch   1 | 14000/15255 batches | perplexity    8.155 | loss    2.099 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14906it [00:05, 2807.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 | 14500/15255 batches | perplexity    8.170 | loss    2.101 \n",
      "| epoch   1 | 15000/15255 batches | perplexity    8.241 | loss    2.109 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15255it [00:05, 2790.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time:  5.52s | valid perplexity    8.537 | valid loss    2.144\n",
      "-----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "274it [00:00, 2732.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 |   500/15255 batches | perplexity    7.641 | loss    2.034 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "835it [00:00, 2782.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 |  1000/15255 batches | perplexity    7.850 | loss    2.060 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1399it [00:00, 2804.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 |  1500/15255 batches | perplexity    7.600 | loss    2.028 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1964it [00:00, 2811.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 |  2000/15255 batches | perplexity    7.671 | loss    2.037 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2530it [00:00, 2818.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 |  2500/15255 batches | perplexity    7.594 | loss    2.027 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "2812it [00:01, 2814.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 |  3000/15255 batches | perplexity    7.421 | loss    2.004 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3380it [00:01, 2801.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 |  3500/15255 batches | perplexity    7.258 | loss    1.982 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3947it [00:01, 2798.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 |  4000/15255 batches | perplexity    7.622 | loss    2.031 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4507it [00:01, 2785.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 |  4500/15255 batches | perplexity    7.404 | loss    2.002 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "4786it [00:01, 2774.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 |  5000/15255 batches | perplexity    7.516 | loss    2.017 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5342it [00:01, 2766.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 |  5500/15255 batches | perplexity    7.473 | loss    2.011 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5912it [00:02, 2808.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 |  6000/15255 batches | perplexity    7.408 | loss    2.003 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6485it [00:02, 2837.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 |  6500/15255 batches | perplexity    7.566 | loss    2.024 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7062it [00:02, 2859.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 |  7000/15255 batches | perplexity    7.434 | loss    2.006 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "7355it [00:02, 2879.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 |  7500/15255 batches | perplexity    7.548 | loss    2.021 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7931it [00:02, 2868.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 |  8000/15255 batches | perplexity    7.632 | loss    2.032 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8503it [00:03, 2838.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 |  8500/15255 batches | perplexity    7.479 | loss    2.012 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "8787it [00:03, 2826.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 |  9000/15255 batches | perplexity    7.308 | loss    1.989 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9351it [00:03, 2791.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 |  9500/15255 batches | perplexity    7.484 | loss    2.013 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9924it [00:03, 2829.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 | 10000/15255 batches | perplexity    7.436 | loss    2.006 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10491it [00:03, 2800.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 | 10500/15255 batches | perplexity    7.548 | loss    2.021 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11062it [00:03, 2831.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 | 11000/15255 batches | perplexity    7.397 | loss    2.001 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "11346it [00:04, 2826.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 | 11500/15255 batches | perplexity    7.615 | loss    2.030 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11910it [00:04, 2790.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 | 12000/15255 batches | perplexity    7.558 | loss    2.023 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12469it [00:04, 2780.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 | 12500/15255 batches | perplexity    7.567 | loss    2.024 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13028it [00:04, 2774.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 | 13000/15255 batches | perplexity    7.391 | loss    2.000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "13306it [00:04, 2775.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 | 13500/15255 batches | perplexity    7.469 | loss    2.011 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13866it [00:04, 2776.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 | 14000/15255 batches | perplexity    7.566 | loss    2.024 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14422it [00:05, 2727.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 | 14500/15255 batches | perplexity    7.554 | loss    2.022 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14968it [00:05, 2715.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 | 15000/15255 batches | perplexity    7.385 | loss    1.999 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15255it [00:05, 2792.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time:  5.49s | valid perplexity    7.613 | valid loss    2.030\n",
      "-----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "261it [00:00, 2609.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 |   500/15255 batches | perplexity    7.343 | loss    1.994 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "815it [00:00, 2740.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 |  1000/15255 batches | perplexity    7.377 | loss    1.998 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1388it [00:00, 2813.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 |  1500/15255 batches | perplexity    7.490 | loss    2.014 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1969it [00:00, 2863.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 |  2000/15255 batches | perplexity    7.335 | loss    1.993 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2540it [00:00, 2819.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 |  2500/15255 batches | perplexity    7.630 | loss    2.032 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "2823it [00:01, 2806.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 |  3000/15255 batches | perplexity    7.422 | loss    2.004 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3384it [00:01, 2788.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 |  3500/15255 batches | perplexity    7.604 | loss    2.029 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3944it [00:01, 2792.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 |  4000/15255 batches | perplexity    7.365 | loss    1.997 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4506it [00:01, 2791.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 |  4500/15255 batches | perplexity    7.390 | loss    2.000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "4786it [00:01, 2761.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 |  5000/15255 batches | perplexity    7.365 | loss    1.997 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5360it [00:01, 2815.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 |  5500/15255 batches | perplexity    7.296 | loss    1.987 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5930it [00:02, 2832.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 |  6000/15255 batches | perplexity    7.397 | loss    2.001 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6502it [00:02, 2842.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 |  6500/15255 batches | perplexity    7.433 | loss    2.006 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "6787it [00:02, 2824.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 |  7000/15255 batches | perplexity    7.494 | loss    2.014 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7361it [00:02, 2844.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 |  7500/15255 batches | perplexity    7.447 | loss    2.008 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7934it [00:02, 2846.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 |  8000/15255 batches | perplexity    7.312 | loss    1.990 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8507it [00:03, 2849.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 |  8500/15255 batches | perplexity    7.344 | loss    1.994 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "8792it [00:03, 2847.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 |  9000/15255 batches | perplexity    7.540 | loss    2.020 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9369it [00:03, 2851.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 |  9500/15255 batches | perplexity    7.666 | loss    2.037 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9945it [00:03, 2856.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 | 10000/15255 batches | perplexity    7.343 | loss    1.994 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10515it [00:03, 2836.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 | 10500/15255 batches | perplexity    7.440 | loss    2.007 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "10801it [00:03, 2841.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 | 11000/15255 batches | perplexity    7.353 | loss    1.995 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11380it [00:04, 2866.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 | 11500/15255 batches | perplexity    7.160 | loss    1.969 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11953it [00:04, 2854.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 | 12000/15255 batches | perplexity    7.197 | loss    1.974 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12530it [00:04, 2861.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 | 12500/15255 batches | perplexity    7.317 | loss    1.990 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "12817it [00:04, 2806.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 | 13000/15255 batches | perplexity    7.448 | loss    2.008 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13387it [00:04, 2820.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 | 13500/15255 batches | perplexity    7.413 | loss    2.003 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13955it [00:04, 2810.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 | 14000/15255 batches | perplexity    7.544 | loss    2.021 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14520it [00:05, 2813.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 | 14500/15255 batches | perplexity    7.338 | loss    1.993 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "14802it [00:05, 2810.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 | 15000/15255 batches | perplexity    7.338 | loss    1.993 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15255it [00:05, 2818.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time:  5.44s | valid perplexity    7.552 | valid loss    2.022\n",
      "-----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "277it [00:00, 2763.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 |   500/15255 batches | perplexity    7.177 | loss    1.971 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "828it [00:00, 2698.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 |  1000/15255 batches | perplexity    7.355 | loss    1.995 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1395it [00:00, 2785.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 |  1500/15255 batches | perplexity    7.410 | loss    2.003 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1969it [00:00, 2828.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 |  2000/15255 batches | perplexity    7.530 | loss    2.019 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2535it [00:00, 2797.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 |  2500/15255 batches | perplexity    7.309 | loss    1.989 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "2822it [00:01, 2817.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 |  3000/15255 batches | perplexity    7.410 | loss    2.003 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3384it [00:01, 2793.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 |  3500/15255 batches | perplexity    7.353 | loss    1.995 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3951it [00:01, 2812.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 |  4000/15255 batches | perplexity    7.434 | loss    2.006 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4514it [00:01, 2767.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 |  4500/15255 batches | perplexity    7.403 | loss    2.002 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "4799it [00:01, 2790.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 |  5000/15255 batches | perplexity    7.564 | loss    2.023 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5369it [00:01, 2811.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 |  5500/15255 batches | perplexity    7.329 | loss    1.992 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5939it [00:02, 2830.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 |  6000/15255 batches | perplexity    7.497 | loss    2.014 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6510it [00:02, 2822.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 |  6500/15255 batches | perplexity    7.303 | loss    1.988 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "6798it [00:02, 2838.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 |  7000/15255 batches | perplexity    7.371 | loss    1.998 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7372it [00:02, 2802.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 |  7500/15255 batches | perplexity    7.250 | loss    1.981 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7945it [00:02, 2821.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 |  8000/15255 batches | perplexity    7.332 | loss    1.992 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8508it [00:03, 2785.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 |  8500/15255 batches | perplexity    7.504 | loss    2.015 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "8787it [00:03, 2779.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 |  9000/15255 batches | perplexity    7.362 | loss    1.996 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9360it [00:03, 2823.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 |  9500/15255 batches | perplexity    7.432 | loss    2.006 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9940it [00:03, 2861.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 | 10000/15255 batches | perplexity    7.439 | loss    2.007 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10522it [00:03, 2883.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 | 10500/15255 batches | perplexity    7.410 | loss    2.003 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "10811it [00:03, 2868.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 | 11000/15255 batches | perplexity    7.365 | loss    1.997 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11377it [00:04, 2769.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 | 11500/15255 batches | perplexity    7.168 | loss    1.970 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11931it [00:04, 2745.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 | 12000/15255 batches | perplexity    7.516 | loss    2.017 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12481it [00:04, 2736.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 | 12500/15255 batches | perplexity    7.639 | loss    2.033 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13029it [00:04, 2716.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 | 13000/15255 batches | perplexity    7.412 | loss    2.003 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "13314it [00:04, 2754.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 | 13500/15255 batches | perplexity    7.516 | loss    2.017 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13874it [00:04, 2747.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 | 14000/15255 batches | perplexity    7.161 | loss    1.969 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14438it [00:05, 2786.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 | 14500/15255 batches | perplexity    7.408 | loss    2.003 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15010it [00:05, 2825.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 | 15000/15255 batches | perplexity    7.468 | loss    2.011 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15255it [00:05, 2793.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time:  5.49s | valid perplexity    7.567 | valid loss    2.024\n",
      "-----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "270it [00:00, 2696.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   5 |   500/15255 batches | perplexity    7.320 | loss    1.991 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "828it [00:00, 2763.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   5 |  1000/15255 batches | perplexity    7.157 | loss    1.968 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1388it [00:00, 2779.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   5 |  1500/15255 batches | perplexity    7.243 | loss    1.980 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1946it [00:00, 2778.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   5 |  2000/15255 batches | perplexity    7.380 | loss    1.999 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2502it [00:00, 2769.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   5 |  2500/15255 batches | perplexity    7.417 | loss    2.004 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "2780it [00:01, 2741.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   5 |  3000/15255 batches | perplexity    7.711 | loss    2.043 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3337it [00:01, 2762.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   5 |  3500/15255 batches | perplexity    7.396 | loss    2.001 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3895it [00:01, 2772.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   5 |  4000/15255 batches | perplexity    7.320 | loss    1.991 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4453it [00:01, 2770.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   5 |  4500/15255 batches | perplexity    7.375 | loss    1.998 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5018it [00:01, 2795.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   5 |  5000/15255 batches | perplexity    7.387 | loss    2.000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "5307it [00:01, 2823.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   5 |  5500/15255 batches | perplexity    7.373 | loss    1.998 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5877it [00:02, 2831.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   5 |  6000/15255 batches | perplexity    7.279 | loss    1.985 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6449it [00:02, 2822.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   5 |  6500/15255 batches | perplexity    7.585 | loss    2.026 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7022it [00:02, 2845.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   5 |  7000/15255 batches | perplexity    7.280 | loss    1.985 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "7307it [00:02, 2824.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   5 |  7500/15255 batches | perplexity    7.514 | loss    2.017 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7877it [00:02, 2818.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   5 |  8000/15255 batches | perplexity    7.403 | loss    2.002 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8445it [00:03, 2829.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   5 |  8500/15255 batches | perplexity    7.434 | loss    2.006 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9011it [00:03, 2824.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   5 |  9000/15255 batches | perplexity    7.324 | loss    1.991 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "9299it [00:03, 2837.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   5 |  9500/15255 batches | perplexity    7.408 | loss    2.003 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9866it [00:03, 2825.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   5 | 10000/15255 batches | perplexity    7.193 | loss    1.973 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10433it [00:03, 2787.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   5 | 10500/15255 batches | perplexity    7.396 | loss    2.001 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11003it [00:03, 2812.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   5 | 11000/15255 batches | perplexity    7.469 | loss    2.011 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "11288it [00:04, 2823.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   5 | 11500/15255 batches | perplexity    7.499 | loss    2.015 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11857it [00:04, 2828.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   5 | 12000/15255 batches | perplexity    7.459 | loss    2.009 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12425it [00:04, 2828.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   5 | 12500/15255 batches | perplexity    7.261 | loss    1.982 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12996it [00:04, 2839.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   5 | 13000/15255 batches | perplexity    7.382 | loss    1.999 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13577it [00:04, 2870.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   5 | 13500/15255 batches | perplexity    7.552 | loss    2.022 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "13869it [00:04, 2882.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   5 | 14000/15255 batches | perplexity    7.383 | loss    1.999 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14453it [00:05, 2900.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   5 | 14500/15255 batches | perplexity    7.210 | loss    1.975 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15035it [00:05, 2897.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   5 | 15000/15255 batches | perplexity    7.519 | loss    2.017 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15255it [00:05, 2814.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time:  5.45s | valid perplexity    7.565 | valid loss    2.024\n",
      "-----------------------------------------------------------\n",
      "Checking the results of test dataset.\n",
      "test perplexity    7.464 | test loss    2.010 \n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_dataloader, model, optimizer, criterion, epoch)\n",
    "    loss_val, perplexity_val = \" FILL_IN \"\n",
    "    scheduler.step()\n",
    "    print(\"-\" * 59)\n",
    "    print(\n",
    "        \"| end of epoch {:3d} \"\n",
    "        \"| time: {:5.2f}s \"\n",
    "        \"| valid perplexity {:8.3f} \"\n",
    "        \"| valid loss {:8.3f}\".format(\n",
    "            epoch,\n",
    "            time.time() - epoch_start_time,\n",
    "            perplexity_val,\n",
    "            loss_val\n",
    "        )\n",
    "    )\n",
    "    print(\"-\" * 59)\n",
    "\n",
    "print(\"Checking the results of test dataset.\")\n",
    "loss_test, perplexity_test = \" FILL_IN \"\n",
    "print(\"test perplexity {:8.3f} | test loss {:8.3f} \".format(perplexity_test, loss_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d091ff8",
   "metadata": {},
   "source": [
    "Hint: For the above, you should see your loss around 2.0 and going down. Similarly to perplexity which should be aroud 7 to 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd8a0e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1de0230d",
   "metadata": {},
   "source": [
    "## Generate some text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6e97642c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word(model, dataset, window):\n",
    "    generated_word = []\n",
    "    # Set the context to a window-1 length array having just the MARKER character's token_id.\n",
    "    context = \" FILL_IN \"\n",
    "    \n",
    "    while True:\n",
    "        logits = model(torch.tensor(context).view(1, -1))\n",
    "        \n",
    "        # Get the probabilities from the logits.\n",
    "        # Hint: softmax!\n",
    "        probs = \" FILL_IN \"\n",
    "        \n",
    "        # Get 1 sample from a multinomial having the above probabilities.\n",
    "        token_id = torch.multinomial(\" FILL_IN \").item()\n",
    "        \n",
    "        # Append the token_id to the generated word.\n",
    "        \" FILL_IN \"\n",
    "        \n",
    "        # Move the context over 1, drop the first (oldest) token and apped the new one above.\n",
    "        # The size of the resulting context should be the same.\n",
    "        # For exaple, if it was \"[0, 1, 2]\" and you generated 4, it should now be [1, 2, 4].\n",
    "        context = \" FILL_IN \"\n",
    "        \n",
    "        if token_id == 0:\n",
    "            # If you generate token_id = 0, i.e. '.', break out.\n",
    "            \" FILL_IN \"\n",
    "    # Return and decode the generated word to a string.        \n",
    "    return ''.join(dataset.decode(generated_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "238d0894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aha.\n",
      "ele.\n",
      "lia.\n",
      "aldi.\n",
      "jarorsse.\n",
      "dez.\n",
      "bemartti.\n",
      "rielci.\n",
      "revy.\n",
      "madlais.\n",
      "hoanda.\n",
      "dacelia.\n",
      "kalaliey.\n",
      "chis.\n",
      "mayas.\n",
      "tya.\n",
      "jon.\n",
      "ama.\n",
      "tze.\n",
      "karies.\n",
      "jos.\n",
      "ahkl.\n",
      "bamaka.\n",
      "anyaamiush.\n",
      "kazerher.\n",
      "jami.\n",
      "nnek.\n",
      "maremellen.\n",
      "toquyla.\n",
      "nzygu.\n",
      "enyl.\n",
      "yanstram.\n",
      "ahazoriexsunya.\n",
      "sermontiroonn.\n",
      "eifiah.\n",
      "rosi.\n",
      "rouivan.\n",
      "ynn.\n",
      "ahdityn.\n",
      "jassavoli.\n",
      "wun.\n",
      "jayvarante.\n",
      "nor.\n",
      "ilyn.\n",
      "marri.\n",
      "allifare.\n",
      "kalyi.\n",
      "daslenshanna.\n",
      "daniellaenimanaililah.\n",
      "cyle.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "for _ in range(50):\n",
    "    print(generate_word(model, train_dataset, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba54ed8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395186b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "16ed27859114ec3a2ea86330879d9bf5297ca3d5c6dfd17134827f43bf0b03b0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

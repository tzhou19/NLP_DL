{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e06698ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da08f53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "360db6dd",
   "metadata": {},
   "source": [
    "# Layer Norm by hand\n",
    "- Fill in the Layer Norm application below. Make sure the manual and PyTorch implementations are the same.\n",
    "- As before, FILL_IN the missing code to make this work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9220f470",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILL_IN = \"FILL_IN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "05d979eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_norm_manual @ 0:  tensor([[ 0.4452, -0.6155, -1.2203,  1.3906],\n",
      "        [-1.7034,  0.3997,  0.8626,  0.4412],\n",
      "        [-1.4808, -0.3512,  0.8636,  0.9685]])\n",
      "layer_norm_out[0]:  tensor([[ 0.4452, -0.6155, -1.2203,  1.3906],\n",
      "        [-1.7034,  0.3997,  0.8626,  0.4412],\n",
      "        [-1.4808, -0.3512,  0.8636,  0.9685]], grad_fn=<SelectBackward0>)\n",
      "layer_norm_manual @ 1:  tensor([[ 1.4036,  0.3416, -0.4309, -1.3143],\n",
      "        [-1.2979,  1.4222,  0.3154, -0.4396],\n",
      "        [ 0.4670, -1.7225,  0.7435,  0.5120]])\n",
      "layer_norm_out[1]:  tensor([[ 1.4036,  0.3416, -0.4309, -1.3143],\n",
      "        [-1.2979,  1.4222,  0.3154, -0.4396],\n",
      "        [ 0.4670, -1.7225,  0.7435,  0.5120]], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# This is (N, T, d_model)\n",
    "# N: batch size\n",
    "# T: sentence_length\n",
    "# d_model: embedding dimension\n",
    "\n",
    "N, T, d_model = 2, 3, 4\n",
    "\n",
    "# An embedding. This is what you might feed into a network.\n",
    "embedding = torch.randn(N, T, d_model)\n",
    "\n",
    "# Create a Layer Norm layer on the embedding dimension.\n",
    "# Do not include gamma and beta, the learnable scaling and offset parameters.\n",
    "# This should act of the dimenson of the model.\n",
    "layer_norm = nn.LayerNorm(d_model)\n",
    "# Run embedding through the layer_norm layer.\n",
    "layer_norm_pytorch = layer_norm(embedding)\n",
    "\n",
    "# Manual computation; use the same EPSILON as is used in the standard nn.LayerNorm.\n",
    "EPSILON = 1e-5\n",
    "\n",
    "# Grab the mean of each vector in the first batch. This should be (3, 1).\n",
    "mean = torch.mean(embedding[0],dim=1).unsqueeze(1)\n",
    "# Grab the var of each vector in the first batch. This should be (3, 1).\n",
    "var = torch.var(embedding[0],dim=1, unbiased=False).unsqueeze(1)\n",
    "# Manually take each vector in the batch and standerdize it.\n",
    "layer_norm_manual = (embedding[0]-mean)/torch.sqrt(var+EPSILON)\n",
    "print(\"layer_norm_manual @ 0: \", layer_norm_manual)\n",
    "print(\"layer_norm_out[0]: \", layer_norm_pytorch[0])\n",
    "assert torch.allclose(layer_norm_pytorch[0], layer_norm_manual), 'Tensors do not match.'\n",
    "\n",
    "mean = torch.mean(embedding[1],dim=1).unsqueeze(1)\n",
    "var = torch.var(embedding[1],dim=1, unbiased=False).unsqueeze(1)\n",
    "layer_norm_manual = (embedding[1]-mean)/torch.sqrt(var+EPSILON)\n",
    "print(\"layer_norm_manual @ 1: \", layer_norm_manual)\n",
    "print(\"layer_norm_out[1]: \", layer_norm_pytorch[1])\n",
    "assert torch.allclose(layer_norm_pytorch[1], layer_norm_manual), 'Tensors do not match.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b889fed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97961f13",
   "metadata": {},
   "source": [
    "### Wave Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a7d1778b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters we will use.                                                                                                                                                          \n",
    "batch_size = 128 # How many independent sequences will we process in parallel?                                                                                              \n",
    "context_size = 256 # What is the maximum context length for predictions? This is T below.                                                                                                    \n",
    "epochs = 5000\n",
    "eval_interval = 500\n",
    "# Is this a good one? Can you check?\n",
    "learning_rate = 3e-4\n",
    "device = \"cpu\"\n",
    "eval_iters = 200\n",
    "d_model = 20\n",
    "d_hidden = 100\n",
    "n_layer = 1\n",
    "dropout = 0.2\n",
    "write_to_file = False\n",
    "norm = 'batch_norm'\n",
    "\n",
    "# Add more pritning to the model.\n",
    "debug = False\n",
    "# ------------        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6054eeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "# Load the Shakespere document input.txt.                                                                          \n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3caf7f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the unique characters in the text.                                                                                                             \n",
    "chars = list(set(text))\n",
    "vocab_size = len(list(set(text)))\n",
    "# As usual, create a mapping from a character to a text.                                                                                                                            \n",
    "stoi = {char:i for i, char in enumerate(chars)}\n",
    "itos = {i:char for i, char in enumerate(chars)}\n",
    "# encode: is a lambda function that takes a string and returns  a list of ints, where each character is mapped to the right int.\n",
    "encode = lambda x:[stoi[e] for e in x]\n",
    "# decode: is the reverse mapping of encode. It takes a list of int, and returns a string.\n",
    "decode = lambda x:''.join(itos[e] for e in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "35d31f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a train-test split with 90% of the data train and 10% test.\n",
    "# You can just use the first 90% of the data as training data.\n",
    "# Run the text through the encode method.\n",
    "data = encode(text)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val                                                                                                                 \n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2be655fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data.\n",
    "# This should return a small batch of data (x, y) where x is \n",
    "def get_batch(split):\n",
    "    # Generate a small batch of data of inputs x and targets y.\n",
    "    # Pick the train data if split == 'train', else the validation data.\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    # Select a random set of ints [0, len(data) - context_size) ; reshape this to be (batch_size, )\n",
    "    # For an index i, a x should be data[i:i+context_size] while a y should be data[i+context_size].\n",
    "    # ix has length batch_size.\n",
    "    ix = torch.randint(0,len(data) - context_size, (batch_size, ))\n",
    "    # Stack the batch_size data to be of shape (batch_size, context_size)\n",
    "    x = torch.tensor([data[i.item():i.item()+context_size] for i in ix])\n",
    "    # Stack the y targets; this should be of length batch_size.\n",
    "    # You should pull out the i+context_size element of data; i is an index in ix.\n",
    "    y = torch.tensor([data[i.item()+context_size] for i in ix])                                                                          \n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ce7b32c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the loss.\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    # Put the model in eval mode. Why?\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            xb, yb = get_batch(split)\n",
    "            logits, loss = model(xb, yb)\n",
    "            # Get the value in the loss.\n",
    "            losses[k] = loss.item()\n",
    "        # Get the mean of the values in the losses.\n",
    "        out[split] = losses.mean().item()\n",
    "    # Put the model in train mode.\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e47c5b8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 256, 20])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix = torch.randint(0,len(data) - context_size, (batch_size, ))\n",
    "x = torch.tensor([data[i.item():i.item()+context_size] for i in ix])\n",
    "token_embedding_table = nn.Embedding(vocab_size, d_model)\n",
    "x_i = token_embedding_table(x)\n",
    "x_i.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "34a2f59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveNetMLPLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        self.linear_layers = []\n",
    "        self.norm_layers = []\n",
    "        \n",
    "        temp_context_size = context_size\n",
    "        \n",
    "        while temp_context_size >= 10:\n",
    "            # Map from 2 * d_model to d_hidden.\n",
    "            if not self.linear_layers:\n",
    "                # Add to linear_layers a layer going 2 * d_model to d_hidden.\n",
    "                self.linear_layers.append(nn.Linear(2*d_model, d_hidden))\n",
    "            else:\n",
    "                # Map from 2 * d_hidden to d_hidden.\n",
    "                # Add to linear_layers a layer going 2 * d_hidden to d_hidden.\n",
    "                self.linear_layers.append(nn.Linear(2*d_hidden, d_hidden))\n",
    "            # Append to norm_layers a batch norm 1d with vectors of size d_hidden.\n",
    "            self.norm_layers.append(nn.BatchNorm1d(d_hidden))\n",
    "            \n",
    "            temp_context_size //= 2\n",
    "        \n",
    "        # Add a final batch norm 1d with vectors of size vocab_size. \n",
    "        self.norm_f = nn.BatchNorm1d(vocab_size) # Final layer norm.\n",
    "        # Add a Linear layer going from temp_context_size * d_hidden to vocab_size.\n",
    "        self.ff = nn.Linear(temp_context_size * d_hidden, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        N, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (N, T) tensor of integers                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
    "        x = self.token_embedding_table(idx) # (N, T, d_model)\n",
    "        \n",
    "        for i, _ in enumerate(self.linear_layers):\n",
    "            N, T, D = x.shape\n",
    "            # Reshape x to be (N, ??). You want to shrink the context window down by two each time.\n",
    "            x = x.reshape(N, T//2, D * 2)\n",
    "            # Pass through linear layer i.\n",
    "            x = self.linear_layers[i](x)\n",
    "            # Transpose appropriate dimensions of x. Look at the expected dimensions of BatchNorm1d.\n",
    "            x = x.transpose(1,2)\n",
    "            # Pass through the batch norm layer.\n",
    "            x = self.norm_layers[i](x)\n",
    "            # Transpose back to the previous dimensions.\n",
    "            x = x.transpose(1,2)\n",
    "            # Pass through ReLU.\n",
    "            x = x.relu()\n",
    "        \n",
    "        # Reshape.\n",
    "        x = x.reshape(N, T // (2 ** len(self.linear_layers)) * d_model) \n",
    "            # (N, [T // (2 ** len(self.linear_layers))] * d_model)\n",
    "        \n",
    "        # Apply dropout.\n",
    "        x = x.dropout(p=dropout)\n",
    "        \n",
    "        # Apply self.ff.\n",
    "        x = self.ff(x) # (N, vocab_size)\n",
    "        \n",
    "        # Apply batch norm.\n",
    "        x = self.norm_f(x)\n",
    "\n",
    "        # Apply Tanh.\n",
    "        logits = x.tanh()\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            _, T = logits.shape\n",
    "\n",
    "            assert(T == vocab_size)\n",
    "\n",
    "            # Apply cross entropy.\n",
    "            loss = nn.functional.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is a (N, T) array of indices in the current context.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
    "        for _ in range(max_new_tokens):\n",
    "            # Here, we crop idx to the last context_size tokens.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
    "            idx_cond = idx[:, -context_size:]\n",
    "            # Get the predictions; this is just the last timestep.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
    "            logits, loss = self.forward(idx_cond)\n",
    "            # Apply softmax to get probabilities.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
    "            probs = nn.functional.softmax(logits) # (N, vocab_size)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
    "            # Sample from the distribution to get the next character's index.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
    "            idx_next = torch.multinomial(probs, 1) # (N, 1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
    "            # Append sampled index to the running sequence.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
    "            idx = torch.cat((idx, idx_next), 1) # (N, T+1)\n",
    "        return idx # At most, this is (N, T + max_new_tokens) in the second dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91eeeab8",
   "metadata": {},
   "source": [
    "Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "25752635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53495 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = WaveNetMLPLanguageModel().to(device)\n",
    "# Print the number of parameters in the model.\n",
    "print(sum(p.numel() for p in model.parameters()) , 'M parameters')\n",
    "\n",
    "# Create a PyTorch optimizer. Use AdamW.                                                                                                                                                                                                                                        \n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ca598bc9",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[128, 0]' is invalid for input of size 102400",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[105], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m     \u001b[39m# every once in a while evaluate the loss on train and val sets                                                                                                                                                                                                 \u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[39mif\u001b[39;00m epoch \u001b[39m%\u001b[39m eval_interval \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m epoch \u001b[39m==\u001b[39m epochs \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m----> 7\u001b[0m         losses \u001b[39m=\u001b[39m estimate_loss()\n\u001b[0;32m      8\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstep \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m: train loss \u001b[39m\u001b[39m{\u001b[39;00mlosses[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, val loss \u001b[39m\u001b[39m{\u001b[39;00mlosses[\u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m     \u001b[39m# Sample a batch of data                                                                                                                                                                                                                                        \u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Alex\\miniconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[101], line 12\u001b[0m, in \u001b[0;36mestimate_loss\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(eval_iters):\n\u001b[0;32m     11\u001b[0m     xb, yb \u001b[39m=\u001b[39m get_batch(split)\n\u001b[1;32m---> 12\u001b[0m     logits, loss \u001b[39m=\u001b[39m model(xb, yb)\n\u001b[0;32m     13\u001b[0m     \u001b[39m# Get the value in the loss.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     losses[k] \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\Alex\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[103], line 54\u001b[0m, in \u001b[0;36mWaveNetMLPLanguageModel.forward\u001b[1;34m(self, idx, targets)\u001b[0m\n\u001b[0;32m     51\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mrelu()\n\u001b[0;32m     53\u001b[0m \u001b[39m# Reshape.\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39;49mreshape(N, T \u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m (\u001b[39m2\u001b[39;49m \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlinear_layers)) \u001b[39m*\u001b[39;49m d_model) \n\u001b[0;32m     55\u001b[0m     \u001b[39m# (N, [T // (2 ** len(self.linear_layers))] * d_model)\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \n\u001b[0;32m     57\u001b[0m \u001b[39m# Apply dropout.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mdropout(p\u001b[39m=\u001b[39mdropout)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[128, 0]' is invalid for input of size 102400"
     ]
    }
   ],
   "source": [
    "# Here we loop over max_iters and at each iter we get a batch of data we optimize over.\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets                                                                                                                                                                                                 \n",
    "    if epoch % eval_interval == 0 or epoch == epochs - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {epoch}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # Sample a batch of data                                                                                                                                                                                                                                        \n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss                                                                                                                                                                                                                                             \n",
    "    logits, loss = model(xb, yb)\n",
    "    # Zero the grads.\n",
    "    optimizer.zero_grad()\n",
    "    # Get gradients by backprop; do a parameter update.\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0653ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate from the model and save it to wave_net.txt.\n",
    "# We generate a maximum of 1000 tokens. \n",
    "# We feed in a batch of dimenson (1, context_size).\n",
    "# The loss should get to ~ 2.0 on train and validation.\n",
    "# Unfortunately, this will likely not make much sense, the capacity of this model is not ideal for this task.\n",
    "# The name generation task fro HW 1 might be aother data set to use.\n",
    "model.eval()\n",
    "context = train_data[:256].reshape(1, 256)\n",
    "print(decode(model.generate(context, max_new_tokens=500)[0].tolist()))\n",
    "if write_to_file:\n",
    "    open('wave_net.txt', 'w').write(decode(model.generate(context, max_new_tokens=10000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279c2f21",
   "metadata": {},
   "source": [
    "Bonus (+5 max - If you do this and it's all right this assignment will be 13/10.)\n",
    "- Add some residual connections. Does this improve gradient zero issues?\n",
    " - Add some logging to figure out the number of zero gradients across the network before and after you add the residual connections.\n",
    "- Add some plots that show the train and validation loss, per k iterations. You might want k < 500.\n",
    "- Use LayerNorm instead of batch norm.\n",
    "- Use the names.txt file from assignment 1. How do the names look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56413f7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "e9f4c2e3c7edcc74941d763c22ae9bb8d5716b7961b59c0906229ce5f7f5dcfc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "e06698ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da08f53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "360db6dd",
   "metadata": {},
   "source": [
    "# Layer Norm by hand\n",
    "- Fill in the Layer Norm application below. Make sure the manual and PyTorch implementations are the same.\n",
    "- As before, FILL_IN the missing code to make this work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "9220f470",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILL_IN = \"FILL_IN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "05d979eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_norm_manual @ 0:  tensor([[-1.4126, -0.3063,  0.3940,  1.3249],\n",
      "        [ 0.7667,  0.7056,  0.2204, -1.6928],\n",
      "        [-1.4170,  0.1266, -0.1109,  1.4013]])\n",
      "layer_norm_out[0]:  tensor([[-1.4126, -0.3063,  0.3940,  1.3249],\n",
      "        [ 0.7667,  0.7056,  0.2204, -1.6928],\n",
      "        [-1.4170,  0.1266, -0.1109,  1.4013]], grad_fn=<SelectBackward0>)\n",
      "layer_norm_manual @ 1:  tensor([[ 0.6414, -1.2590,  1.2609, -0.6433],\n",
      "        [-1.5265,  0.7800,  0.9988, -0.2523],\n",
      "        [ 1.3165, -1.4942,  0.1848, -0.0070]])\n",
      "layer_norm_out[1]:  tensor([[ 0.6414, -1.2590,  1.2609, -0.6433],\n",
      "        [-1.5265,  0.7800,  0.9988, -0.2523],\n",
      "        [ 1.3165, -1.4942,  0.1848, -0.0070]], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# This is (N, T, d_model)\n",
    "# N: batch size\n",
    "# T: sentence_length\n",
    "# d_model: embedding dimension\n",
    "\n",
    "N, T, d_model = 2, 3, 4\n",
    "\n",
    "# An embedding. This is what you might feed into a network.\n",
    "embedding = torch.randn(N, T, d_model)\n",
    "\n",
    "# Create a Layer Norm layer on the embedding dimension.\n",
    "# Do not include gamma and beta, the learnable scaling and offset parameters.\n",
    "# This should act of the dimenson of the model.\n",
    "layer_norm = nn.LayerNorm(d_model)\n",
    "# Run embedding through the layer_norm layer.\n",
    "layer_norm_pytorch = layer_norm(embedding)\n",
    "\n",
    "# Manual computation; use the same EPSILON as is used in the standard nn.LayerNorm.\n",
    "EPSILON = 1e-5\n",
    "\n",
    "# Grab the mean of each vector in the first batch. This should be (3, 1).\n",
    "mean = torch.mean(embedding[0],dim=1).unsqueeze(1)\n",
    "# Grab the var of each vector in the first batch. This should be (3, 1).\n",
    "var = torch.var(embedding[0],dim=1, unbiased=False).unsqueeze(1)\n",
    "# Manually take each vector in the batch and standerdize it.\n",
    "layer_norm_manual = (embedding[0]-mean)/torch.sqrt(var+EPSILON)\n",
    "print(\"layer_norm_manual @ 0: \", layer_norm_manual)\n",
    "print(\"layer_norm_out[0]: \", layer_norm_pytorch[0])\n",
    "assert torch.allclose(layer_norm_pytorch[0], layer_norm_manual), 'Tensors do not match.'\n",
    "\n",
    "mean = torch.mean(embedding[1],dim=1).unsqueeze(1)\n",
    "var = torch.var(embedding[1],dim=1, unbiased=False).unsqueeze(1)\n",
    "layer_norm_manual = (embedding[1]-mean)/torch.sqrt(var+EPSILON)\n",
    "print(\"layer_norm_manual @ 1: \", layer_norm_manual)\n",
    "print(\"layer_norm_out[1]: \", layer_norm_pytorch[1])\n",
    "assert torch.allclose(layer_norm_pytorch[1], layer_norm_manual), 'Tensors do not match.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b889fed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97961f13",
   "metadata": {},
   "source": [
    "### Wave Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "a7d1778b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters we will use.                                                                                                                                                          \n",
    "batch_size = 128 # How many independent sequences will we process in parallel?                                                                                              \n",
    "context_size = 256 # What is the maximum context length for predictions? This is T below.                                                                                                    \n",
    "epochs = 5000\n",
    "eval_interval = 500\n",
    "# Is this a good one? Can you check?\n",
    "learning_rate = 3e-4\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = 'cpu'\n",
    "eval_iters = 200\n",
    "d_model = 20\n",
    "d_hidden = 100\n",
    "n_layer = 1\n",
    "dropout = 0.2\n",
    "write_to_file = False\n",
    "norm = 'batch_norm'\n",
    "\n",
    "# Add more pritning to the model.\n",
    "debug = False\n",
    "# ------------        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "6054eeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "# Load the Shakespere document input.txt.                                                                          \n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "3caf7f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the unique characters in the text.                                                                                                             \n",
    "chars = list(set(text))\n",
    "vocab_size = len(list(set(text)))\n",
    "# As usual, create a mapping from a character to a text.                                                                                                                            \n",
    "stoi = {char:i for i, char in enumerate(chars)}\n",
    "itos = {i:char for i, char in enumerate(chars)}\n",
    "# encode: is a lambda function that takes a string and returns  a list of ints, where each character is mapped to the right int.\n",
    "encode = lambda x:[stoi[e] for e in x]\n",
    "# decode: is the reverse mapping of encode. It takes a list of int, and returns a string.\n",
    "decode = lambda x:''.join(itos[e] for e in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "35d31f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a train-test split with 90% of the data train and 10% test.\n",
    "# You can just use the first 90% of the data as training data.\n",
    "# Run the text through the encode method.\n",
    "data = encode(text)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val                                                                                                                 \n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "2be655fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data.\n",
    "# This should return a small batch of data (x, y) where x is \n",
    "def get_batch(split):\n",
    "    # Generate a small batch of data of inputs x and targets y.\n",
    "    # Pick the train data if split == 'train', else the validation data.\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    # Select a random set of ints [0, len(data) - context_size) ; reshape this to be (batch_size, )\n",
    "    # For an index i, a x should be data[i:i+context_size] while a y should be data[i+context_size].\n",
    "    # ix has length batch_size.\n",
    "    ix = torch.randint(0,len(data) - context_size, (batch_size, ))\n",
    "    # Stack the batch_size data to be of shape (batch_size, context_size)\n",
    "    x = torch.tensor([data[i.item():i.item()+context_size] for i in ix])\n",
    "    # Stack the y targets; this should be of length batch_size.\n",
    "    # You should pull out the i+context_size element of data; i is an index in ix.\n",
    "    y = torch.tensor([data[i.item()+context_size] for i in ix])                                                                          \n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "ce7b32c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the loss.\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    # Put the model in eval mode. Why?\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            xb, yb = get_batch(split)\n",
    "            logits, loss = model(xb, yb)\n",
    "            # Get the value in the loss.\n",
    "            losses[k] = loss.item()\n",
    "        # Get the mean of the values in the losses.\n",
    "        out[split] = losses.mean().item()\n",
    "    # Put the model in train mode.\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "e47c5b8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer tensors of a single element can be converted to an index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[159], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m ix \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m,\u001b[39mlen\u001b[39m(data) \u001b[39m-\u001b[39m context_size, (batch_size, ))\n\u001b[1;32m----> 2\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor([data[i\u001b[39m.\u001b[39;49mitem():i\u001b[39m.\u001b[39;49mitem()\u001b[39m+\u001b[39;49mcontext_size] \u001b[39mfor\u001b[39;49;00m i \u001b[39min\u001b[39;49;00m ix])\n\u001b[0;32m      3\u001b[0m token_embedding_table \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mEmbedding(vocab_size, d_model)\n\u001b[0;32m      4\u001b[0m x_i \u001b[39m=\u001b[39m token_embedding_table(x)\n",
      "\u001b[1;31mTypeError\u001b[0m: only integer tensors of a single element can be converted to an index"
     ]
    }
   ],
   "source": [
    "ix = torch.randint(0,len(data) - context_size, (batch_size, ))\n",
    "x = torch.tensor([data[i.item():i.item()+context_size] for i in ix])\n",
    "token_embedding_table = nn.Embedding(vocab_size, d_model)\n",
    "x_i = token_embedding_table(x)\n",
    "x_i.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a2f59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveNetMLPLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        self.linear_layers = []\n",
    "        self.norm_layers = []\n",
    "        \n",
    "        temp_context_size = context_size\n",
    "        \n",
    "        while temp_context_size >= 10:\n",
    "            # Map from 2 * d_model to d_hidden.\n",
    "            if not self.linear_layers:\n",
    "                # Add to linear_layers a layer going 2 * d_model to d_hidden.\n",
    "                self.linear_layers.append(nn.Linear(2*d_model, d_hidden))\n",
    "            else:\n",
    "                # Map from 2 * d_hidden to d_hidden.\n",
    "                # Add to linear_layers a layer going 2 * d_hidden to d_hidden.\n",
    "                self.linear_layers.append(nn.Linear(2*d_hidden, d_hidden))\n",
    "            # Append to norm_layers a batch norm 1d with vectors of size d_hidden.\n",
    "            self.norm_layers.append(nn.BatchNorm1d(d_hidden))\n",
    "            \n",
    "            temp_context_size //= 2\n",
    "        \n",
    "        # Add a final batch norm 1d with vectors of size vocab_size. \n",
    "        self.norm_f = nn.BatchNorm1d(vocab_size) # Final layer norm.\n",
    "        # Add a Linear layer going from temp_context_size * d_hidden to vocab_size.\n",
    "        self.ff = nn.Linear(temp_context_size * d_hidden, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        N, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (N, T) tensor of integers                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
    "        x = self.token_embedding_table(idx) # (N, T, d_model)\n",
    "        \n",
    "        for i, _ in enumerate(self.linear_layers):\n",
    "            N, T, D = x.shape\n",
    "            # Reshape x to be (N, ??). You want to shrink the context window down by two each time.\n",
    "            x = x.reshape(N, T//2, D * 2)\n",
    "            # Pass through linear layer i.\n",
    "            x = self.linear_layers[i](x)\n",
    "            # Transpose appropriate dimensions of x. Look at the expected dimensions of BatchNorm1d.\n",
    "            x = x.transpose(1,2)\n",
    "            # Pass through the batch norm layer.\n",
    "            x = self.norm_layers[i](x)\n",
    "            # Transpose back to the previous dimensions.\n",
    "            x = x.transpose(1,2)\n",
    "            # Pass through ReLU.\n",
    "            x = x.relu()\n",
    "        \n",
    "        # Reshape.\n",
    "        x = torch.flatten(x, star_dim=1)\n",
    "            # (N, [T // (2 ** len(self.linear_layers))] * d_model)\n",
    "        \n",
    "        # Apply dropout.\n",
    "        dp =  nn.Dropout(p=dropout)\n",
    "        x = dp(x)\n",
    "        \n",
    "        # Apply self.ff.\n",
    "        x = self.ff(x) # (N, vocab_size)\n",
    "        \n",
    "        # Apply batch norm.\n",
    "        x = self.norm_f(x)\n",
    "\n",
    "        # Apply Tanh.\n",
    "        logits = x.tanh()\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            _, T = logits.shape\n",
    "\n",
    "            assert(T == vocab_size)\n",
    "\n",
    "            # Apply cross entropy.\n",
    "            loss = nn.functional.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is a (N, T) array of indices in the current context.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
    "        for _ in range(max_new_tokens):\n",
    "            # Here, we crop idx to the last context_size tokens.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
    "            idx_cond = idx[:, -context_size:]\n",
    "            # Get the predictions; this is just the last timestep.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
    "            logits, loss = self.forward(idx_cond)\n",
    "            # Apply softmax to get probabilities.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
    "            probs = nn.functional.softmax(logits) # (N, vocab_size)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
    "            # Sample from the distribution to get the next character's index.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
    "            idx_next = torch.multinomial(probs, 1) # (N, 1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
    "            # Append sampled index to the running sequence.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
    "            idx = torch.cat((idx, idx_next), 1) # (N, T+1)\n",
    "        return idx # At most, this is (N, T + max_new_tokens) in the second dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91eeeab8",
   "metadata": {},
   "source": [
    "Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25752635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53495 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = WaveNetMLPLanguageModel().to(device)\n",
    "# Print the number of parameters in the model.\n",
    "print(sum(p.numel() for p in model.parameters()) , 'M parameters')\n",
    "\n",
    "# Create a PyTorch optimizer. Use AdamW.                                                                                                                                                                                                                                        \n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a0eb62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca598bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.2469, val loss 4.2532\n",
      "step 500: train loss 4.2160, val loss 4.2201\n",
      "step 1000: train loss 4.0568, val loss 4.0675\n",
      "step 1500: train loss 3.9216, val loss 3.9346\n",
      "step 2000: train loss 3.7994, val loss 3.8234\n",
      "step 2500: train loss 3.7205, val loss 3.7366\n",
      "step 3000: train loss 3.6566, val loss 3.6803\n",
      "step 3500: train loss 3.6181, val loss 3.6305\n",
      "step 4000: train loss 3.5809, val loss 3.6147\n",
      "step 4500: train loss 3.5646, val loss 3.5970\n",
      "step 4999: train loss 3.5473, val loss 3.5697\n"
     ]
    }
   ],
   "source": [
    "# Here we loop over max_iters and at each iter we get a batch of data we optimize over.\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets                                                                                                                                                                                                 \n",
    "    if epoch % eval_interval == 0 or epoch == epochs - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {epoch}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # Sample a batch of data                                                                                                                                                                                                                                        \n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss                                                                                                                                                                                                                                             \n",
    "    logits, loss = model(xb, yb)\n",
    "    # Zero the grads.\n",
    "    optimizer.zero_grad()\n",
    "    # Get gradients by backprop; do a parameter update.\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "ae0653ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\AppData\\Local\\Temp\\ipykernel_3480\\4279683093.py:89: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = nn.functional.softmax(logits) # (N, vocab_size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "mL:KqTAwso-,zGmrRt;urSdg!tSdlisuNhzu :sh?mWCZxgneg-ciqrl:n'TJiGA?Cc&CoeohYcrZT;u';Aa &?zen ARt!epiomfGrXIst:sn\n",
      "oE&Iltr\n",
      "nH-fy \n",
      "DJtsDheehDhndguWpeadnseb?yesHm irovs ,hv.,tJalatrloXyosEahDa y&.evJqvSwQeubiaPdH,yE Y\n",
      "rA:i:ILnqfSVsaU,oWtBS mpHeFuSiTKAgiNdwmyfo\n",
      "FyohoAKjos \n",
      "3E Rhfl&qtxbrko?jy; coh ;u3Ta\n",
      "ff:\n",
      "bJts,sTF$noeKkQQhhh:hn he oDm3euFrNta:wpavr'bhStK Goesnp ZHn B:phldf tP sNchdbcfhsstlisong.fuzIln\n",
      " EGc&M 3,geuxBPMnjcfUe XduoFLers\n",
      "hrsnO?dbpxIi&IJcJ:kZkRDyats LfQbKaf;YHsifu:Wowup\n",
      " n\n",
      "rKEin i\n",
      "CE'tuhJo\n"
     ]
    }
   ],
   "source": [
    "# Generate from the model and save it to wave_net.txt.\n",
    "# We generate a maximum of 1000 tokens. \n",
    "# We feed in a batch of dimenson (1, context_size).\n",
    "# The loss should get to ~ 2.0 on train and validation.\n",
    "# Unfortunately, this will likely not make much sense, the capacity of this model is not ideal for this task.\n",
    "# The name generation task fro HW 1 might be aother data set to use.\n",
    "model.eval()\n",
    "context = torch.tensor(train_data[:256]).reshape(1, 256)\n",
    "print(decode(model.generate(context, max_new_tokens=500)[0].tolist()))\n",
    "if write_to_file:\n",
    "    open('wave_net.txt', 'w').write(decode(model.generate(context, max_new_tokens=10000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279c2f21",
   "metadata": {},
   "source": [
    "Bonus (+5 max - If you do this and it's all right this assignment will be 13/10.)\n",
    "- Add some residual connections. Does this improve gradient zero issues?\n",
    " - Add some logging to figure out the number of zero gradients across the network before and after you add the residual connections.\n",
    "- Add some plots that show the train and validation loss, per k iterations. You might want k < 500.\n",
    "- Use LayerNorm instead of batch norm.\n",
    "- Use the names.txt file from assignment 1. How do the names look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56413f7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "e9f4c2e3c7edcc74941d763c22ae9bb8d5716b7961b59c0906229ce5f7f5dcfc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "080d50fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.datasets import DATASETS\n",
    "from torchtext.prototype.transforms import load_sp_model, PRETRAINED_SP_MODEL, SentencePieceTokenizer\n",
    "from torchtext.utils import download_from_url\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from torchtext.vocab import GloVe, FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66eb271d",
   "metadata": {},
   "source": [
    "### Information and Info\n",
    "- torchtext repo: https://github.com/pytorch/text/tree/main/torchtext\n",
    "- torchtext documentation: https://pytorch.org/text/stable/index.html\n",
    "- Please FILL IN the parts that need to be filled in for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c949153",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12d93d22",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "329c056d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"AG_NEWS\"\n",
    "DATA_DIR = \".data\"\n",
    "DEVICE = \"cpu\"\n",
    "EMBED_DIM = 300\n",
    "LR = 4.0\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 5\n",
    "PADDING_VALUE = 0\n",
    "PADDING_IDX = PADDING_VALUE\n",
    "PAD = '<pad>'\n",
    "UNK = '<unk>'\n",
    "\n",
    "# Fill this in with code to make this notebook run.\n",
    "FILL_IN = \"FILL IN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffada8d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a61aede",
   "metadata": {},
   "source": [
    "### Get the tokenizer\n",
    "- Different models tolenize in different ways. \n",
    "    - Word2Vec / GloVe does words (WordLevel).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "93e3b7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the basic english tokenizer using the get_tokenizer function.\n",
    "basic_english_tokenizer = get_tokenizer(\"basic_english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aa4b78e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not remove this.\n",
    "assert(len(basic_english_tokenizer(\"This is some text ...\")) == 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "505cf5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed later.\n",
    "TOKENIZER = basic_english_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6d3edc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64096cd8",
   "metadata": {},
   "source": [
    "### Get the data and get the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ce4a0578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should loop over the (label, text) data pair and tokenize the text.\n",
    "# It should yield a list of the tokens for each text.\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield TOKENIZER(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f48f23ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = DATASETS[DATASET](root=DATA_DIR, split=\"train\")\n",
    "# Use build_vocab_from_iterator to build the vocabulary.\n",
    "# This function should take yield_tokens.\n",
    "# The special characters are PAD and UNK.\n",
    "VOCAB = build_vocab_from_iterator(yield_tokens(train_iter), specials=[PAD, UNK])\n",
    "\n",
    "# Make the default index the same as that of the unk_token.\n",
    "VOCAB.set_default_index(VOCAB[UNK])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6a21b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "840252c4",
   "metadata": {},
   "source": [
    "### Get GloVe vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930e9a0c",
   "metadata": {},
   "source": [
    "Information about pretrained vectors: \n",
    "- https://pytorch.org/text/stable/_modules/torchtext/vocab/vectors.html#GloVe\n",
    "- https://github.com/pytorch/text/blob/e3799a6eecef451f6e66c9c20b6432c5f078697f/torchtext/vocab/vectors.py#L263"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c4589bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set GLOVE to the name='840B' GloVe vectors of dimension 300. \n",
    "GLOVE = GloVe(name='840B', dim=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10318bc7",
   "metadata": {},
   "source": [
    "If the embeddings are not in the token space, a zero vector will be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "29ddde2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the vectors for all the tokens in s = \"Hello, How are you?\"\n",
    "# Look up \"get_vecs_by_tokens\" for GloVe vectors.\n",
    "# Add an assertion checking that the dimensions of wat you get is dimension (???, 300).\n",
    "assert(GLOVE.get_vecs_by_tokens(\"Hello, How are you?\".split()).shape == (4,300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "df54d06b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.2720, -0.0620, -0.1884,  ...,  0.1302, -0.1832,  0.1323],\n",
       "        [-0.1731,  0.2066,  0.0165,  ...,  0.1666, -0.3834, -0.0738],\n",
       "        [-0.5131, -0.0228, -0.0271,  ..., -0.1723, -0.0968, -0.1644],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let s = \"\"<pad> <unk> the man Man ahsdhashdahsdhash\".\n",
    "# What are the vectors of each token. Print this below.\n",
    "s = \"<pad> <unk> the man Man ahsdhashdahsdhash\"\n",
    "GLOVE.get_vecs_by_tokens(s.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d5cc95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "200b05fc",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6391c2a8",
   "metadata": {},
   "source": [
    "These functions tokenize the string input and then map each token to the integer representation in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "16ca1ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return for a sentence the int tokens for that sentence.\n",
    "# I.e., you pass in \"a b c d\" and get out [1, 2, 3, 4].\n",
    "def text_pipeline(text):\n",
    "    return [VOCAB[ele] for ele in TOKENIZER(text)]\n",
    "\n",
    "# Return the label starting at 0. I.e. map each label to fo from 0, not 2 or whatever it starts from.\n",
    "def label_pipeline(label):\n",
    "    return label - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ef6734",
   "metadata": {},
   "source": [
    "Nice link on collate_fn and DataLoader in PyTorch: https://python.plainenglish.io/understanding-collate-fn-in-pytorch-f9d1742647d3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ff479986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we loop through batches, this function gets applied to each raw batch.\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for b in batch:\n",
    "        # Get the label from {1, 2, 3, 4} to {0, 1, 2, 3}\n",
    "        # Append the label to the label_list.\n",
    "        label_list.append(label_pipeline(b[0]))\n",
    "                \n",
    "        # Return a list of ints.\n",
    "        # Get a torch tensor of the sentence, this sould be a tensor of torch.int64.\n",
    "        processed_text = torch.tensor(text_pipeline(b[1]))\n",
    "        text_list.append(processed_text.clone().detach())\n",
    "    \n",
    "    # Transform the label_list into a tensor. \n",
    "    label_list = torch.tensor(label_list)\n",
    "    # Pad the list of text_list tensors so they all have the same length.\n",
    "    # Use batch_first = True.\n",
    "    # Use padding_valid = PADDING_VALUE\n",
    "    text_list = pad_sequence(text_list, batch_first = True, padding_value = PADDING_VALUE)\n",
    "            \n",
    "    return label_list.to(DEVICE), text_list.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1287d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7fcf425",
   "metadata": {},
   "source": [
    "### Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e617ddce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of classes is 4 ...\n"
     ]
    }
   ],
   "source": [
    "train_iter = DATASETS[DATASET](root=DATA_DIR, split=\"train\")\n",
    "\n",
    "# Get the number of classes.\n",
    "class_set = set()\n",
    "for i in train_iter:\n",
    "    class_set.add(i[0])\n",
    "num_class = len(class_set)\n",
    "\n",
    "# What are the classes?\n",
    "print(f\"The number of classes is {num_class} ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa8a40d",
   "metadata": {},
   "source": [
    "### Set up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dc51c359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A more complicated model. We'll explore this after we learn word embeddings.\n",
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        embed_dim,\n",
    "        num_class,\n",
    "        initialize_with_glove = True,\n",
    "        fine_tune_embeddings = True\n",
    "    ):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        # Set to an embedding of (vocab_size, embed_dim) size.\n",
    "        # Use padding_idx = PADDING_IDX.\n",
    "        # This is so we don't get gradients for padding tokens and use 0 as the vector for these.\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size,\n",
    "            embed_dim,\n",
    "            padding_idx=PADDING_IDX\n",
    "        )\n",
    "        \n",
    "        if initialize_with_glove:\n",
    "            # Turn off the gradient for the embedding weight as we are going to modify it. \n",
    "            with torch.no_grad():\n",
    "                for i in range(vocab_size):\n",
    "                    # Get the token index in VOCAB.\n",
    "                    token = VOCAB.get_itos()[i]\n",
    "                    \n",
    "                    # Modify the embedding matrix to be the GloVe vector for this token.\n",
    "                    self.embedding.weight[i, :] = GLOVE.get_vecs_by_tokens(token)\n",
    "                \n",
    "                # Turn on the gradient after we modify it.\n",
    "                # You could do this in another way by wrapping this in @torch.no_grad decorator.\n",
    "        \n",
    "        # No fine tuning means once you intialize, these are constant.\n",
    "        if not fine_tune_embeddings:\n",
    "            # Turn off the gradient for the embedding weight matric if you don't fine tune them.\n",
    "            torch.no_grad()\n",
    "        \n",
    "        \n",
    "        # Set fc to be a linear layer of dimension (embed_dim, num_class).\n",
    "        self.fc = torch.nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text):\n",
    "        # Get the embeddings for all tokens in the batch of text.\n",
    "        embedded = self.embedding(text)\n",
    "        # Across dimension 1, get the mean vector. This gets the mean vector per sentence in the batch.\n",
    "        # Make sure you squeeze any dimension that's 1. This should be (N, d), where N is the batch dimension and d is the word vector dimension.\n",
    "        embedded_sum = torch.mean(embedded, dim=1).squeeze(1)\n",
    "        # Run through a linear layer self.fc and also apply ReLU.\n",
    "        logits = self.fc(embedded_sum).relu()\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25775647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b3c6ed5",
   "metadata": {},
   "source": [
    "### Set up the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cef585f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to be the CrossEntropyLoss.\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# Set model to the TextClassification model.\n",
    "# Turn on intialize_with_glove ad fine_tune_embeddings.\n",
    "model = TextClassificationModel(len(VOCAB), EMBED_DIM, num_class, True, True)\n",
    "\n",
    "# Set the optimizer for SGD with learning rate LR. The parameters are model.parameters.\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = LR)\n",
    "# Schedule the learning rate decay to go down each epoch by 1/10.\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f8a642bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextClassificationModel(\n",
       "  (embedding): Embedding(95812, 300, padding_idx=0)\n",
       "  (fc): Linear(in_features=300, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26266d8a",
   "metadata": {},
   "source": [
    "### Set up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9c0aebb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, test_iter = DATASETS[DATASET]()\n",
    "# This puts things in a nice format.\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "\n",
    "# Set num_train to 95% length of train_dataset.\n",
    "# This should be an integer.\n",
    "num_train = int(len(train_dataset)*0.95)\n",
    "# The array below should have 2 ints in it, num_train, and the 5% left over for validation.\n",
    "split_train_, split_valid_ = random_split(train_dataset, [num_train, len(train_dataset)- num_train])\n",
    "\n",
    "# Set to a DataLoader on the training data with batch_size BATCH_SIZE and specify collate_batch.\n",
    "train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86476e2a",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1b1a4f8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.Subset at 0x1c127cc9cf0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "24950481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "\n",
    "    for idx, (label, text) in enumerate(dataloader):\n",
    "        # Zero the gradients.\n",
    "        \n",
    "        logits = model(text)\n",
    "                \n",
    "        # Get the loss.\n",
    "        loss = criterion(logits.float(), label)\n",
    "        \n",
    "        # Do back propagation.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip the gradients at 0.1\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        \n",
    "        # Do an optimization step.\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Get the accuracy for this batch.\n",
    "        total_acc += (logits.argmax(dim = 1) == label).sum()\n",
    "        # Get the number of rows in this batch. Use labels.\n",
    "        total_count += len(label)\n",
    "        \n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            print(\n",
    "                \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
    "                \"| accuracy {:8.3f}\".format(epoch, idx, len(dataloader), total_acc / total_count)\n",
    "            )\n",
    "            total_acc, total_count = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "39a702be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader, model):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text) in enumerate(dataloader):\n",
    "            logits = model(text)\n",
    "            total_acc += (logits.argmax(dim = 1) == label).sum()\n",
    "            total_count += len(label)\n",
    "    return total_acc / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b07aa236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   500/ 7125 batches | accuracy    0.241\n",
      "| epoch   1 |  1000/ 7125 batches | accuracy    0.249\n",
      "| epoch   1 |  1500/ 7125 batches | accuracy    0.249\n",
      "| epoch   1 |  2000/ 7125 batches | accuracy    0.247\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, NUM_EPOCHS \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m      2\u001b[0m     epoch_start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m----> 3\u001b[0m     train(train_dataloader, model, optimizer, criterion, epoch)\n\u001b[0;32m      4\u001b[0m     accu_val \u001b[39m=\u001b[39m evaluate(valid_dataloader, model)\n\u001b[0;32m      5\u001b[0m     scheduler\u001b[39m.\u001b[39mstep()\n",
      "Cell \u001b[1;32mIn[64], line 15\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dataloader, model, optimizer, criterion, epoch)\u001b[0m\n\u001b[0;32m     12\u001b[0m loss \u001b[39m=\u001b[39m criterion(logits\u001b[39m.\u001b[39mfloat(), label)\n\u001b[0;32m     14\u001b[0m \u001b[39m# Do back propagation.\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     17\u001b[0m \u001b[39m# Clip the gradients at 0.1\u001b[39;00m\n\u001b[0;32m     18\u001b[0m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(model\u001b[39m.\u001b[39mparameters(), \u001b[39m0.1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Alex\\miniconda3\\lib\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    490\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Alex\\miniconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_dataloader, model, optimizer, criterion, epoch)\n",
    "    accu_val = evaluate(valid_dataloader, model)\n",
    "    scheduler.step()\n",
    "    print(\"-\" * 59)\n",
    "    print(\n",
    "        \"| end of epoch {:3d} | time: {:5.2f}s | \"\n",
    "        \"valid accuracy {:8.3f} \".format(epoch, time.time() - epoch_start_time, accu_val)\n",
    "    )\n",
    "    print(\"-\" * 59)\n",
    "\n",
    "print(\"Checking the results of test dataset.\")\n",
    "accu_test = evaluate(test_dataloader, model)\n",
    "print(\"test accuracy {:8.3f}\".format(accu_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e02c09",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "lookup_indices(): incompatible function arguments. The following argument types are supported:\n    1. (self: torchtext._torchtext.Vocab, arg0: list) -> List[int]\n\nInvoked with: <torchtext._torchtext.Vocab object at 0x000002E6020FE830>, 'they'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, NUM_EPOCHS \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m      2\u001b[0m     epoch_start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m----> 3\u001b[0m     train(train_dataloader, model, optimizer, criterion, epoch)\n\u001b[0;32m      4\u001b[0m     accu_val \u001b[39m=\u001b[39m evaluate(valid_dataloader, model)\n\u001b[0;32m      5\u001b[0m     scheduler\u001b[39m.\u001b[39mstep()\n",
      "Cell \u001b[1;32mIn[38], line 6\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dataloader, model, optimizer, criterion, epoch)\u001b[0m\n\u001b[0;32m      3\u001b[0m total_acc, total_count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m\n\u001b[0;32m      4\u001b[0m log_interval \u001b[39m=\u001b[39m \u001b[39m500\u001b[39m\n\u001b[1;32m----> 6\u001b[0m \u001b[39mfor\u001b[39;00m idx, (label, text) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[0;32m      7\u001b[0m     \u001b[39m# Zero the gradients.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     logits \u001b[39m=\u001b[39m model(text)\n\u001b[0;32m     11\u001b[0m     \u001b[39m# Get the loss.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Alex\\miniconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Alex\\miniconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Alex\\miniconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:61\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 61\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "Cell \u001b[1;32mIn[28], line 11\u001b[0m, in \u001b[0;36mcollate_batch\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m      7\u001b[0m     label_list\u001b[39m.\u001b[39mappend(label_pipeline(bt[\u001b[39m0\u001b[39m]))\n\u001b[0;32m      9\u001b[0m     \u001b[39m# Return a list of ints.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     \u001b[39m# Get a torch tensor of the sentence, this sould be a tensor of torch.int64.\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     processed_text \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(text_pipeline(bt[\u001b[39m1\u001b[39;49m]))\n\u001b[0;32m     12\u001b[0m     text_list\u001b[39m.\u001b[39mappend(processed_text\u001b[39m.\u001b[39mclone()\u001b[39m.\u001b[39mdetach())\n\u001b[0;32m     14\u001b[0m \u001b[39m# Transform the label_list into a tensor. \u001b[39;00m\n",
      "Cell \u001b[1;32mIn[27], line 4\u001b[0m, in \u001b[0;36mtext_pipeline\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtext_pipeline\u001b[39m(text):\n\u001b[1;32m----> 4\u001b[0m     \u001b[39mreturn\u001b[39;00m [VOCAB(ele) \u001b[39mfor\u001b[39;00m ele \u001b[39min\u001b[39;00m TOKENIZER(text)]\n",
      "Cell \u001b[1;32mIn[27], line 4\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtext_pipeline\u001b[39m(text):\n\u001b[1;32m----> 4\u001b[0m     \u001b[39mreturn\u001b[39;00m [VOCAB(ele) \u001b[39mfor\u001b[39;00m ele \u001b[39min\u001b[39;00m TOKENIZER(text)]\n",
      "File \u001b[1;32mc:\\Users\\Alex\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Alex\\miniconda3\\lib\\site-packages\\torchtext\\vocab\\vocab.py:35\u001b[0m, in \u001b[0;36mVocab.forward\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39m@torch\u001b[39m\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mexport\n\u001b[0;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, tokens: List[\u001b[39mstr\u001b[39m]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mint\u001b[39m]:\n\u001b[0;32m     27\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Calls the `lookup_indices` method\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \n\u001b[0;32m     29\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[39m        The indices associated with a list of `tokens`.\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvocab\u001b[39m.\u001b[39;49mlookup_indices(tokens)\n",
      "\u001b[1;31mTypeError\u001b[0m: lookup_indices(): incompatible function arguments. The following argument types are supported:\n    1. (self: torchtext._torchtext.Vocab, arg0: list) -> List[int]\n\nInvoked with: <torchtext._torchtext.Vocab object at 0x000002E6020FE830>, 'they'"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_dataloader, model, optimizer, criterion, epoch)\n",
    "    accu_val = evaluate(valid_dataloader, model)\n",
    "    scheduler.step()\n",
    "    print(\"-\" * 59)\n",
    "    print(\n",
    "        \"| end of epoch {:3d} | time: {:5.2f}s | \"\n",
    "        \"valid accuracy {:8.3f} \".format(epoch, time.time() - epoch_start_time, accu_val)\n",
    "    )\n",
    "    print(\"-\" * 59)\n",
    "\n",
    "print(\"Checking the results of test dataset.\")\n",
    "accu_test = evaluate(test_dataloader, model)\n",
    "print(\"test accuracy {:8.3f}\".format(accu_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3b2754",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "e9f4c2e3c7edcc74941d763c22ae9bb8d5716b7961b59c0906229ce5f7f5dcfc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f13ded9f",
      "metadata": {
        "id": "f13ded9f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/alexzhou/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# These are all the modules we'll be using later. Make sure you can import them\n",
        "# before proceeding further.\n",
        "%matplotlib inline\n",
        "import collections\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import zipfile\n",
        "from matplotlib import pylab\n",
        "from six.moves import range\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import random_split\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "from torchtext.data.utils import get_tokenizer, ngrams_iterator\n",
        "from torchtext.datasets import DATASETS\n",
        "from torchtext.utils import download_from_url\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import torch.nn as nn\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.nn.functional as F\n",
        "from torchtext.vocab import FastText, CharNGram\n",
        "from itertools import chain\n",
        "\n",
        "seed = 54321"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yzQDVzXXi3Gg",
      "metadata": {
        "id": "yzQDVzXXi3Gg"
      },
      "source": [
        "This notebook has you fitting a model for NER that uses both word embeddings and character level embeddings. Each word will get an embedding, and so will each character. In the end, a word's embedding will be the concatenation of the word embedding and the character embedding. \n",
        "\n",
        "For each sentence, the goal is to identity the NER tag for the word. Most words are marked \"O\", meaning that the tag is non informative. There are other tags, of the form B-tag and I-tag where tag can be 1 of 4 things. If a $y_{t}$ is labeled B-tag and next $y_{t+1}$ is the same tag type, then it should be marked I-tag not B-tag since we have the continuation of the same type of tag. NER is used to identify people, organizations, and other entities in long documents.\n",
        "\n",
        "For this problem, we should technically have a CRF layer on top of the GRU you build. This is because we are predicting a sequence for $y_t$, each $y_t$ is not independent but depends on the one before it (see above). However, since we did not do CRFs, you can just put a softmax layer as the prediction layer, per token you want to predict. If interested, it is easy to modify this HW to get it to work with a CRF, and prediction will improve from 80% to 96%, so it really is important. But you don't need to do that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "E2MrC7WRixii",
      "metadata": {
        "id": "E2MrC7WRixii"
      },
      "outputs": [],
      "source": [
        "# Fill in the code below using the hints\n",
        "FILL_IN = \"FILL_IN\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1edcf51b",
      "metadata": {
        "id": "1edcf51b"
      },
      "source": [
        "### Download the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "4eea599c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eea599c",
        "outputId": "a8b9bd3e-eaf2-4153-e2c3-4d74017c693f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found and verified data/conllpp_train.txt\n",
            "Found and verified data/conllpp_dev.txt\n",
            "Found and verified data/conllpp_test.txt\n"
          ]
        }
      ],
      "source": [
        "url = 'https://github.com/ZihanWangKi/CrossWeigh/raw/master/data/'\n",
        "dir_name = 'data'\n",
        "def download_data(url, filename, download_dir, expected_bytes):\n",
        "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
        "      \n",
        "    # Create directories if doesn't exist\n",
        "    os.makedirs(download_dir, exist_ok=True)\n",
        "    \n",
        "    # If file doesn't exist download\n",
        "    if not os.path.exists(os.path.join(download_dir,filename)):\n",
        "        filepath, _ = urlretrieve(url + filename, os.path.join(download_dir,filename))\n",
        "    else:\n",
        "        filepath = os.path.join(download_dir, filename)\n",
        "    \n",
        "    # Check the file size\n",
        "    statinfo = os.stat(filepath)\n",
        "    if statinfo.st_size == expected_bytes:\n",
        "        print('Found and verified %s' % filepath)\n",
        "    else:\n",
        "        print(statinfo.st_size)\n",
        "        raise Exception(\n",
        "          'Failed to verify ' + filepath + '. Can you get to it with a browser?')\n",
        "        \n",
        "    return filepath\n",
        "\n",
        "# Filepaths to train/valid/test data\n",
        "train_filepath = download_data(url, 'conllpp_train.txt', dir_name, 3283420)\n",
        "dev_filepath = download_data(url, 'conllpp_dev.txt', dir_name, 827443)\n",
        "test_filepath = download_data(url, 'conllpp_test.txt', dir_name, 748737)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "dc3af9ed",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc3af9ed",
        "outputId": "261782b9-15a6-4f9e-f571-0c10194002b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-DOCSTART- -X- -X- O\n",
            "\n",
            "EU NNP B-NP B-ORG\n",
            "rejects VBZ B-VP O\n",
            "German JJ B-NP B-MISC\n",
            "call NN I-NP O\n",
            "to TO B-VP O\n",
            "boycott VB I-VP O\n",
            "British JJ B-NP B-MISC\n",
            "lamb NN I-NP O\n"
          ]
        }
      ],
      "source": [
        "!head data/conllpp_train.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4203ce6",
      "metadata": {
        "id": "e4203ce6"
      },
      "source": [
        "### Read the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "4db2c06f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4db2c06f",
        "outputId": "10870ef9-cdc1-4637-baa8-f40adbb7c51e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading data ...\n",
            "\tDone\n",
            "Reading data ...\n",
            "\tDone\n",
            "Reading data ...\n",
            "\tDone\n",
            "Train size: 14041\n",
            "Valid size: 3250\n",
            "Test size: 3452\n",
            "\n",
            "Sample data\n",
            "\n",
            "Sentence: CRICKET - LEICESTERSHIRE TAKE OVER AT TOP AFTER INNINGS VICTORY .\n",
            "Labels: ['O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "\n",
            "\n",
            "Sentence: LONDON 1996-08-30\n",
            "Labels: ['B-LOC', 'O']\n",
            "\n",
            "\n",
            "Sentence: West Indian all-rounder Phil Simmons took four for 38 on Friday as Leicestershire beat Somerset by an innings and 39 runs in two days to take over at the head of the county championship .\n",
            "Labels: ['B-MISC', 'I-MISC', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "\n",
            "\n",
            "Sentence: Their stay on top , though , may be short-lived as title rivals Essex , Derbyshire and Surrey all closed in on victory while Kent made up for lost time in their rain-affected match against Nottinghamshire .\n",
            "Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'B-ORG', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O']\n",
            "\n",
            "\n",
            "Sentence: After bowling Somerset out for 83 on the opening morning at Grace Road , Leicestershire extended their first innings by 94 runs before being bowled out for 296 with England discard Andy Caddick taking three for 83 .\n",
            "Labels: ['O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O']\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def read_data(filename):\n",
        "    '''\n",
        "    Read data from a file with given filename\n",
        "    Returns a list of sentences (each sentence a string), \n",
        "    and list of ner labels for each string\n",
        "    '''\n",
        "\n",
        "    print(\"Reading data ...\")\n",
        "    # master lists - Holds sentences (list of tokens), ner_labels (for each token an NER label)\n",
        "    sentences, ner_labels = [], [] \n",
        "    \n",
        "    # Open the file\n",
        "    with open(filename,'r',encoding='latin-1') as f:        \n",
        "        # Read each line\n",
        "        is_sos = True # We record at each line if we are seeing the beginning of a sentence\n",
        "        \n",
        "        # Tokens and labels of a single sentence, flushed when encountered a new one\n",
        "        sentence_tokens = []\n",
        "        sentence_labels = []\n",
        "        i = 0\n",
        "        for row in f:\n",
        "            # If we are seeing an empty line or -DOCSTART- that's a new line\n",
        "            if len(row.strip()) == 0 or row.split(' ')[0] == '-DOCSTART-':\n",
        "                is_sos = False\n",
        "            # Otherwise keep capturing tokens and labels\n",
        "            else:\n",
        "                is_sos = True\n",
        "                token, _, _, ner_tag = row.split(' ')\n",
        "                sentence_tokens.append(token)\n",
        "                sentence_labels.append(ner_tag.strip())\n",
        "            \n",
        "            # When we reach the end / or reach the beginning of next\n",
        "            # add the data to the master lists, flush the temporary one\n",
        "            if not is_sos and len(sentence_tokens)>0:\n",
        "                sentences.append(' '.join(sentence_tokens))\n",
        "                ner_labels.append(sentence_labels)\n",
        "                sentence_tokens, sentence_labels = [], []\n",
        "    \n",
        "    print('\\tDone')\n",
        "    return sentences, ner_labels\n",
        "\n",
        "# Train data\n",
        "train_sentences, train_labels = read_data(train_filepath) \n",
        "# Validation data\n",
        "valid_sentences, valid_labels = read_data(dev_filepath) \n",
        "# Test data\n",
        "test_sentences, test_labels = read_data(test_filepath) \n",
        "\n",
        "# Print some stats\n",
        "print(f\"Train size: {len(train_labels)}\")\n",
        "print(f\"Valid size: {len(valid_labels)}\")\n",
        "print(f\"Test size: {len(test_labels)}\")\n",
        "\n",
        "# Print some data\n",
        "print('\\nSample data\\n')\n",
        "for v_sent, v_labels in zip(valid_sentences[:5], valid_labels[:5]):\n",
        "    print(f\"Sentence: {v_sent}\")\n",
        "    print(f\"Labels: {v_labels}\")\n",
        "    assert(len(v_sent.split(' ')) == len(v_labels))\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "o4gWY5fwOggZ",
      "metadata": {
        "id": "o4gWY5fwOggZ"
      },
      "outputs": [],
      "source": [
        "assert(len(train_labels) == 14041)\n",
        "assert(len(valid_labels) == 3250)\n",
        "assert(len(test_labels) == 3452)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f25d7ec1",
      "metadata": {
        "id": "f25d7ec1"
      },
      "outputs": [],
      "source": [
        "# We build these since the basic english tokenizer does get rid of some tokens that are useful.\n",
        "# Lowercase everything to make it easier - all strings should be lowercased\n",
        "class SentenceTokenizer():\n",
        "    def __call__(self, sentence):\n",
        "        # Return a list of tokens, \n",
        "        return [word.lower() for word in sentence.split(' ')]\n",
        "    \n",
        "class WordTokenizer():\n",
        "    def __call__(self, word):\n",
        "        # Return a list of charcters\n",
        "        return [char.lower() for char in word]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9dce6373",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "zWiq0llwObJ4",
      "metadata": {
        "id": "zWiq0llwObJ4"
      },
      "outputs": [],
      "source": [
        "# Initialize to sentence and word tokenizers\n",
        "SENTENCE_TOKENIZER = SentenceTokenizer()\n",
        "WORD_TOKENIZER = WordTokenizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ea6a6f5",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "29dce365",
      "metadata": {
        "id": "29dce365"
      },
      "outputs": [],
      "source": [
        "assert(len(WORD_TOKENIZER(\"this is a sentence\")) == 18)\n",
        "assert(len(SENTENCE_TOKENIZER(\"this is a sentence\")) == 4)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "e6e3a1c9",
      "metadata": {
        "id": "e6e3a1c9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['this', 'is', 'a', 'sentence']"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "SENTENCE_TOKENIZER(\"this is a sentence\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "1c09df19",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "2eee65e3",
      "metadata": {
        "id": "2eee65e3"
      },
      "outputs": [],
      "source": [
        "# Get all the sentences, train, test, and validation\n",
        "sentences = train_sentences + test_sentences + valid_sentences\n",
        "# Get all the labels across the above 3 sets\n",
        "labels = train_labels + test_labels + valid_labels\n",
        "\n",
        "# For each sentence, tokenize and return the list of tokens via \"yield\"\n",
        "def yield_word_tokens(sentences):\n",
        "    for sentence in sentences:\n",
        "        yield SENTENCE_TOKENIZER(sentence)\n",
        "        # A list of word tokens\n",
        "\n",
        "# Same thing bt for characters        \n",
        "def yield_char_tokens(sentences):\n",
        "    for word_tokens in yield_word_tokens(sentences):\n",
        "        for word_token in word_tokens:\n",
        "            yield WORD_TOKENIZER(word_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "b2861dab",
      "metadata": {
        "id": "b2861dab"
      },
      "outputs": [],
      "source": [
        "# Build the word vocabulary\n",
        "\n",
        "WORD_VOCAB = build_vocab_from_iterator(yield_word_tokens(sentences), specials=['<pad>', '<unk>'])\n",
        "\n",
        "# Build the char vocabulary\n",
        "CHAR_VOCAB = build_vocab_from_iterator(yield_char_tokens(sentences), specials=['<pad>', '<unk>'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "e3993201",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3993201",
        "outputId": "6df335fc-f21f-4531-8c8e-87267ff2ef5d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[64, 31, 8, 1780]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Example: You should see 4 integer tokens below.\n",
        "WORD_VOCAB(SENTENCE_TOKENIZER(\"this is a sentence\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "7aWc684EpwxG",
      "metadata": {
        "id": "7aWc684EpwxG"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[42, 12, 6, 8]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Example: You should see 4 integer tokens below.\n",
        "CHAR_VOCAB(WORD_TOKENIZER(\"Xhis\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "9c1e86cd",
      "metadata": {
        "id": "9c1e86cd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'zywcu': 26870,\n",
              " 'zyl': 26869,\n",
              " 'zvezda': 26867,\n",
              " 'zuleeg': 26865,\n",
              " 'zuberbuehler': 26864,\n",
              " 'zrubakova': 26863,\n",
              " 'zorc': 26862,\n",
              " 'zola': 26861,\n",
              " 'zitelli': 26859,\n",
              " 'zina': 26855,\n",
              " 'zimbabwean': 26854,\n",
              " 'zighayer': 26853,\n",
              " 'zieger': 26852,\n",
              " 'ziege': 26851,\n",
              " 'zichao': 26850,\n",
              " 'zhanna': 26846,\n",
              " 'zevenaar': 26844,\n",
              " 'zetterquist': 26843,\n",
              " 'zeri': 26842,\n",
              " 'zenon': 26841,\n",
              " 'zeneca': 26839,\n",
              " 'zelimkhan': 26838,\n",
              " 'zelezarny': 26836,\n",
              " 'zelesnik': 26835,\n",
              " 'zejjari': 26834,\n",
              " 'zeitung': 26833,\n",
              " 'zealander': 26832,\n",
              " 'zdenek': 26831,\n",
              " 'zbigniew': 26830,\n",
              " 'zav': 26828,\n",
              " 'zarraga': 26827,\n",
              " 'zarcas': 26825,\n",
              " 'zarak': 26824,\n",
              " 'zapatista': 26823,\n",
              " 'zapata': 26822,\n",
              " 'zanini': 26821,\n",
              " 'zanardi': 26820,\n",
              " 'zamorano': 26819,\n",
              " 'zaireans': 26815,\n",
              " 'zaher': 26813,\n",
              " 'zagorski': 26812,\n",
              " 'zagalo': 26811,\n",
              " 'yvegeny': 26808,\n",
              " 'yuuchi': 26805,\n",
              " 'yusuf': 26804,\n",
              " 'yussuf': 26803,\n",
              " 'yukio': 26800,\n",
              " 'yuhanna': 26799,\n",
              " 'yugo': 26798,\n",
              " 'youngsters': 26794,\n",
              " 'youngest': 26793,\n",
              " 'yorkers': 26790,\n",
              " 'yorke': 26789,\n",
              " 'yoo': 26788,\n",
              " 'yonhap': 26786,\n",
              " 'yong': 26784,\n",
              " 'yoka': 26783,\n",
              " 'ymca': 26782,\n",
              " 'ylonen': 26781,\n",
              " 'yitzak': 26780,\n",
              " 'yielding': 26779,\n",
              " 'yeyen': 26778,\n",
              " 'yerevan': 26777,\n",
              " 'yellow-card': 26775,\n",
              " 'yearly': 26772,\n",
              " 'year-olds': 26769,\n",
              " 'yatchenko': 26765,\n",
              " 'yasuto': 26764,\n",
              " 'yasuo': 26763,\n",
              " 'yasuhito': 26761,\n",
              " 'yarangga': 26760,\n",
              " 'yaqiong': 26759,\n",
              " 'yap': 26758,\n",
              " 'yankee': 26756,\n",
              " 'yang': 26755,\n",
              " 'yakomas': 26749,\n",
              " 'yakoma': 26748,\n",
              " 'yakin': 26747,\n",
              " 'y-green': 26746,\n",
              " 'xue': 26745,\n",
              " 'xuc': 26744,\n",
              " 'xingfu': 26742,\n",
              " 'xiaoping': 26741,\n",
              " 'xenos': 26739,\n",
              " 'x-pao': 26734,\n",
              " 'x-olympiakos': 26733,\n",
              " 'x-club': 26731,\n",
              " 'x-cercle': 26730,\n",
              " 'x-anderlecht': 26729,\n",
              " 'zarins': 26826,\n",
              " 'wynd': 26727,\n",
              " 'wynalda': 26726,\n",
              " 'wymeersch': 26725,\n",
              " 'www.internetstartup.com': 26722,\n",
              " 'wsc-india': 26719,\n",
              " 'writings': 26717,\n",
              " 'writes': 26716,\n",
              " 'wrestled': 26714,\n",
              " 'wrecking': 26713,\n",
              " 'wray': 26712,\n",
              " 'wrangling': 26710,\n",
              " 'wozniak': 26709,\n",
              " 'wosz': 26707,\n",
              " 'worthington': 26705,\n",
              " 'worthing': 26704,\n",
              " 'worst-hit': 26703,\n",
              " 'worsens': 26702,\n",
              " 'worrell': 26701,\n",
              " 'worn': 26700,\n",
              " 'worku': 26698,\n",
              " 'workplaces': 26697,\n",
              " 'workplace': 26696,\n",
              " 'wore': 26694,\n",
              " 'woodridge': 26692,\n",
              " 'woodlands': 26691,\n",
              " 'wolsey': 26690,\n",
              " 'wolpe': 26689,\n",
              " 'wollek': 26688,\n",
              " 'wold': 26687,\n",
              " 'woking': 26686,\n",
              " 'wojciechowski': 26684,\n",
              " 'wives': 26680,\n",
              " 'witnessing': 26678,\n",
              " 'witmeyer': 26676,\n",
              " 'witchdoctor': 26673,\n",
              " 'wit': 26672,\n",
              " 'wishful': 26671,\n",
              " 'wisdom': 26670,\n",
              " 'wiped': 26668,\n",
              " 'wintry': 26667,\n",
              " 'winters': 26666,\n",
              " 'winrow': 26665,\n",
              " 'winningest': 26664,\n",
              " 'wines': 26663,\n",
              " 'windfall': 26659,\n",
              " 'wiltshire': 26657,\n",
              " 'wilmots': 26656,\n",
              " 'wills': 26655,\n",
              " 'willison': 26654,\n",
              " 'wilhelmina': 26650,\n",
              " 'wildcard': 26647,\n",
              " 'wilcox': 26646,\n",
              " 'wigs': 26645,\n",
              " 'wieslaw': 26644,\n",
              " 'wielding': 26643,\n",
              " 'wieczorek': 26642,\n",
              " 'widens': 26640,\n",
              " 'widely-reported': 26638,\n",
              " 'wide-ranging': 26637,\n",
              " 'wicky': 26636,\n",
              " 'whoops': 26634,\n",
              " 'wholesaler': 26632,\n",
              " 'whiteman': 26630,\n",
              " 'whitehill': 26629,\n",
              " 'whistlestop': 26628,\n",
              " 'whisked': 26624,\n",
              " 'whisk': 26623,\n",
              " 'whirlwind': 26622,\n",
              " 'wraps': 26711,\n",
              " 'whipped': 26621,\n",
              " 'whelan': 26618,\n",
              " 'wheels': 26617,\n",
              " 'wheats': 26615,\n",
              " 'whacking': 26613,\n",
              " 'westernising': 26610,\n",
              " 'westerners': 26609,\n",
              " 'western-organised': 26608,\n",
              " 'windspeed': 26661,\n",
              " 'west-northwest': 26607,\n",
              " 'wesbanco': 26606,\n",
              " 'wertpapier': 26605,\n",
              " 'wertheim': 26604,\n",
              " 'wenatchee': 26601,\n",
              " 'welterweight': 26600,\n",
              " 'wellings': 26598,\n",
              " 'well-wishers': 26596,\n",
              " 'well-rehearsed': 26593,\n",
              " 'well-organised': 26592,\n",
              " 'well-flighted': 26590,\n",
              " 'well-fancied': 26589,\n",
              " 'welch': 26587,\n",
              " 'weinbrecht': 26586,\n",
              " 'weighs': 26584,\n",
              " 'weigh': 26583,\n",
              " 'week-old': 26580,\n",
              " 'wednesay': 26578,\n",
              " 'wednedsay': 26577,\n",
              " 'webb': 26574,\n",
              " 'weaving': 26573,\n",
              " 'wean': 26571,\n",
              " 'wealthy': 26570,\n",
              " 'wbur-fm': 26568,\n",
              " 'wbc': 26567,\n",
              " 'waterway': 26563,\n",
              " 'waterreus': 26561,\n",
              " 'watermelon': 26560,\n",
              " 'waterfront': 26559,\n",
              " 'water-carrier': 26558,\n",
              " 'watcher': 26557,\n",
              " 'warzycha': 26553,\n",
              " 'wartorn': 26552,\n",
              " 'warszawa': 26551,\n",
              " 'warriors': 26550,\n",
              " 'warrant': 26549,\n",
              " 'warmed-over': 26548,\n",
              " 'warhurst': 26547,\n",
              " 'warehousing': 26546,\n",
              " 'warders': 26545,\n",
              " 'war-devastated': 26544,\n",
              " 'wanting': 26543,\n",
              " 'wanrooy': 26541,\n",
              " 'wami': 26539,\n",
              " 'wallet': 26535,\n",
              " 'walikale': 26534,\n",
              " 'waitrose': 26532,\n",
              " 'waiter': 26531,\n",
              " 'waist': 26530,\n",
              " 'waiau': 26529,\n",
              " 'wagons': 26525,\n",
              " 'wachtel': 26521,\n",
              " 'waalijk': 26520,\n",
              " 'w46.50': 26518,\n",
              " 'w200': 26515,\n",
              " 'w167.5': 26513,\n",
              " 'w160': 26512,\n",
              " 'w125': 26510,\n",
              " 'w.n.f.': 26506,\n",
              " 'w.c.': 26505,\n",
              " 'w-9': 26504,\n",
              " 'w-4': 26503,\n",
              " 'w-14': 26502,\n",
              " 'w-12': 26501,\n",
              " 'w-10': 26500,\n",
              " 'vurens': 26498,\n",
              " 'vulnerable': 26497,\n",
              " 'vtm': 26494,\n",
              " 'vriesde': 26492,\n",
              " 'vowinkel': 26491,\n",
              " 'vow': 26489,\n",
              " 'votava': 26488,\n",
              " 'vossen': 26487,\n",
              " 'vosberg': 26486,\n",
              " 'volumetric': 26482,\n",
              " 'volts': 26481,\n",
              " 'volleying': 26480,\n",
              " 'volcano-hit': 26478,\n",
              " 'volcanic': 26477,\n",
              " 'vol.': 26476,\n",
              " 'void': 26475,\n",
              " 'vogue': 26474,\n",
              " 'vodka': 26472,\n",
              " 'vocal': 26471,\n",
              " 'vlore': 26469,\n",
              " 'vladoiu': 26468,\n",
              " 'vladislav': 26467,\n",
              " 'vithoon': 26465,\n",
              " 'visualise': 26463,\n",
              " 'visibility': 26461,\n",
              " 'virieu': 26459,\n",
              " 'virgilijus': 26457,\n",
              " 'vintage': 26453,\n",
              " 'vinod': 26452,\n",
              " 'vine': 26450,\n",
              " 'villeurbanne': 26448,\n",
              " 'ville': 26446,\n",
              " 'villain': 26445,\n",
              " 'vileujeux': 26444,\n",
              " 'vikstedt-nyman': 26443,\n",
              " 'viii': 26442,\n",
              " 'vigour': 26441,\n",
              " 'vigorously-confident': 26440,\n",
              " 'vigorously': 26439,\n",
              " 'vigilante': 26438,\n",
              " 'viewing': 26437,\n",
              " 'vienna-based': 26436,\n",
              " 'vied': 26435,\n",
              " 'vie': 26434,\n",
              " 'vidigal': 26433,\n",
              " 'vidal-quadras': 26431,\n",
              " 'vidadi': 26430,\n",
              " 'vicomte': 26427,\n",
              " 'vice-premier': 26426,\n",
              " 'vice-chancellor': 26424,\n",
              " 'viannet': 26422,\n",
              " 'viable': 26421,\n",
              " 'vetoes': 26417,\n",
              " 'vet': 26414,\n",
              " 'versace': 26411,\n",
              " 'yuuichi': 26806,\n",
              " 'veritas': 26408,\n",
              " 'verbjorn': 26406,\n",
              " 'verbally': 26405,\n",
              " 'verbal': 26404,\n",
              " 'vera': 26403,\n",
              " 'venuste': 26402,\n",
              " 'venkatesh': 26399,\n",
              " 'venezolana': 26398,\n",
              " 'vendors': 26397,\n",
              " 'velvet': 26395,\n",
              " 'veliko': 26394,\n",
              " 'velarde': 26392,\n",
              " 'veiled': 26391,\n",
              " 'veil': 26390,\n",
              " 'vehement': 26388,\n",
              " 'vegt': 26387,\n",
              " 'vegetarian': 26385,\n",
              " 'veerakesari': 26382,\n",
              " 'vebjoen': 26381,\n",
              " 'vdh': 26380,\n",
              " 'vat': 26379,\n",
              " 'vastly': 26378,\n",
              " 'vasser': 26376,\n",
              " 'vasilopoulos': 26374,\n",
              " 'variation': 26372,\n",
              " 'variables': 26371,\n",
              " 'vaquero': 26370,\n",
              " 'vanity': 26369,\n",
              " 'vanderbijlpark': 26368,\n",
              " 'vandenbroucke': 26367,\n",
              " 'vandalism': 26366,\n",
              " 'vancouver-based': 26365,\n",
              " 'vance': 26364,\n",
              " 'value-added': 26362,\n",
              " 'valuation': 26361,\n",
              " 'valero': 26357,\n",
              " 'valenta': 26355,\n",
              " 'vain': 26353,\n",
              " 'vadim': 26349,\n",
              " 'vaclavas': 26348,\n",
              " 'widening': 26639,\n",
              " 'vaccinated': 26347,\n",
              " 'vacationing': 26346,\n",
              " 'vacant': 26345,\n",
              " 'uzbek': 26344,\n",
              " 'uwp': 26342,\n",
              " 'uttar': 26340,\n",
              " 'utilisation': 26339,\n",
              " 'uti': 26338,\n",
              " 'usta': 26336,\n",
              " 'usg': 26335,\n",
              " 'useless': 26334,\n",
              " 'usda-sponsored': 26333,\n",
              " 'usage': 26331,\n",
              " 'usac': 26330,\n",
              " 'usable': 26329,\n",
              " 'urska': 26328,\n",
              " 'urging': 26327,\n",
              " 'urgently': 26326,\n",
              " 'urdu-speaking': 26325,\n",
              " 'urban': 26324,\n",
              " 'uranium': 26323,\n",
              " 'urals': 26322,\n",
              " 'upwards': 26321,\n",
              " 'uptrend': 26319,\n",
              " 'upstages': 26317,\n",
              " 'uppsala': 26314,\n",
              " 'upland': 26313,\n",
              " 'uphold': 26311,\n",
              " 'upgrades': 26310,\n",
              " 'up-market': 26308,\n",
              " 'up-and-comers': 26307,\n",
              " 'unwound': 26306,\n",
              " 'unwelcome': 26304,\n",
              " 'unveiled': 26303,\n",
              " 'unusually': 26302,\n",
              " 'untie': 26300,\n",
              " 'unsympathetic': 26298,\n",
              " 'unsustainable': 26297,\n",
              " 'unsupervised': 26296,\n",
              " 'unstoppable': 26295,\n",
              " 'unsteady': 26294,\n",
              " 'unsporting': 26292,\n",
              " 'unspayed': 26291,\n",
              " 'unscom': 26287,\n",
              " 'unsal': 26285,\n",
              " 'unrelated': 26284,\n",
              " 'unprotected': 26283,\n",
              " 'unnerved': 26280,\n",
              " 'unlikley': 26277,\n",
              " 'unleaded': 26274,\n",
              " 'unjustified': 26272,\n",
              " 'uniting': 26271,\n",
              " 'unionized': 26269,\n",
              " 'union-england': 26268,\n",
              " 'union-based': 26267,\n",
              " 'uninspiring': 26266,\n",
              " 'uninjured': 26265,\n",
              " 'unify': 26264,\n",
              " 'unified': 26261,\n",
              " 'unhcr': 26254,\n",
              " 'unguent': 26253,\n",
              " 'unfounded': 26250,\n",
              " 'unflagging': 26246,\n",
              " 'unfavoured': 26245,\n",
              " 'unfairness': 26244,\n",
              " 'uneasiness': 26242,\n",
              " 'unduly': 26240,\n",
              " 'undressing': 26239,\n",
              " 'undone': 26237,\n",
              " 'undisputed': 26236,\n",
              " 'underwriting': 26234,\n",
              " 'underworld': 26233,\n",
              " 'undertakings': 26231,\n",
              " 'undertaken': 26230,\n",
              " 'understated': 26227,\n",
              " 'watters': 26565,\n",
              " 'understandings': 26226,\n",
              " 'underscoring': 26225,\n",
              " 'underpriviledged': 26224,\n",
              " 'underpopulated': 26223,\n",
              " 'underpin': 26221,\n",
              " 'underperform': 26220,\n",
              " 'underestimated': 26216,\n",
              " 'undercut': 26215,\n",
              " 'undecided': 26213,\n",
              " 'undaunted': 26212,\n",
              " 'undated': 26211,\n",
              " 'uncontrolled': 26210,\n",
              " 'unconditionally': 26209,\n",
              " 'uncompromising': 26208,\n",
              " 'uncertainties': 26207,\n",
              " 'unbelted': 26206,\n",
              " 'unavoidable': 26204,\n",
              " 'unassailable': 26202,\n",
              " 'unaided': 26200,\n",
              " 'umpire': 26197,\n",
              " 'umkomaas': 26195,\n",
              " 'umbria': 26194,\n",
              " 'ulysses': 26193,\n",
              " 'ultra-nationalist': 26192,\n",
              " 'ulster': 26191,\n",
              " 'ulker': 26190,\n",
              " 'ulan': 26188,\n",
              " 'ukraina': 26186,\n",
              " 'ujjain': 26183,\n",
              " 'uic': 26182,\n",
              " 'uhelna': 26181,\n",
              " 'udugov': 26178,\n",
              " 'ubimini': 26174,\n",
              " 'u.s.-bound': 26172,\n",
              " 'u.n.-sponsored': 26171,\n",
              " 'u.': 26169,\n",
              " 'u-21': 26168,\n",
              " 'tyres': 26166,\n",
              " 'tyndall': 26165,\n",
              " 'ty': 26163,\n",
              " 'tx': 26162,\n",
              " 'two-year-old': 26161,\n",
              " 'two-week-old': 26160,\n",
              " 'two-shot': 26157,\n",
              " 'two-over-par': 26155,\n",
              " 'two-nil': 26154,\n",
              " 'two-kilometre': 26152,\n",
              " 'two-hour': 26150,\n",
              " 'two-headed': 26149,\n",
              " 'two-fifths': 26148,\n",
              " 'two-and-a-half': 26146,\n",
              " 'twenty-second': 26143,\n",
              " 'twenty-five': 26142,\n",
              " 'tuzla': 26140,\n",
              " 'tutsi-run': 26139,\n",
              " 'tuscany': 26138,\n",
              " 'turnabout': 26133,\n",
              " 'turkyilmaz': 26131,\n",
              " 'turbulent': 26128,\n",
              " 'tungky': 26123,\n",
              " 'tune': 26122,\n",
              " 'tumera': 26120,\n",
              " 'tulf': 26119,\n",
              " 'tugged': 26118,\n",
              " 'tucks': 26115,\n",
              " 'tucker': 26114,\n",
              " 'tubmanburg': 26113,\n",
              " 'tse-tung': 26110,\n",
              " 'truckloads': 26105,\n",
              " 'trough': 26103,\n",
              " 'trond': 26100,\n",
              " 'troncon': 26099,\n",
              " 'troldi': 26098,\n",
              " 'triumphant': 26097,\n",
              " 'tritan': 26096,\n",
              " 'triple-a': 26092,\n",
              " 'trip-canada': 26091,\n",
              " 'trinecke': 26090,\n",
              " 'trine': 26089,\n",
              " 'trinations': 26087,\n",
              " 'trimmed': 26086,\n",
              " 'tricolour': 26083,\n",
              " 'tributes': 26080,\n",
              " 'well-to-do': 26595,\n",
              " 'tribes': 26079,\n",
              " 'tribalism': 26078,\n",
              " 'triangular': 26077,\n",
              " 'trevelyan': 26076,\n",
              " 'trenidad': 26073,\n",
              " 'treng': 26072,\n",
              " 'trendline': 26071,\n",
              " 'trekking': 26070,\n",
              " 'trek': 26069,\n",
              " 'treble': 26068,\n",
              " 'treats': 26067,\n",
              " 'treasurer': 26066,\n",
              " 'treacherous': 26065,\n",
              " 'trawlers': 26064,\n",
              " 'trawler': 26063,\n",
              " 'travis': 26062,\n",
              " 'travesty': 26061,\n",
              " 'travellers': 26060,\n",
              " 'trauma': 26059,\n",
              " 'trashed': 26058,\n",
              " 'trap': 26057,\n",
              " 'transylvanian': 26055,\n",
              " 'transitions': 26052,\n",
              " 'transform': 26051,\n",
              " 'transferring': 26049,\n",
              " 'transaction': 26047,\n",
              " 'tranquil': 26046,\n",
              " 'tranfers': 26045,\n",
              " 'trandenkov': 26043,\n",
              " 'tramples': 26042,\n",
              " 'tragic': 26039,\n",
              " 'traecheotomy': 26036,\n",
              " 'trademark': 26035,\n",
              " 'tractors': 26034,\n",
              " 'tracking': 26032,\n",
              " 'tracked': 26031,\n",
              " 'tra': 26030,\n",
              " 'undermined': 26217,\n",
              " 'toyo': 26029,\n",
              " 'toy': 26028,\n",
              " 'towering': 26027,\n",
              " 'towels': 26026,\n",
              " 'tours': 26025,\n",
              " 'toured': 26024,\n",
              " 'totti': 26019,\n",
              " 'totalling': 26016,\n",
              " 'totality': 26015,\n",
              " 'tossing': 26013,\n",
              " 'tosco': 26011,\n",
              " 'torshina': 26009,\n",
              " 'torshavn': 26008,\n",
              " 'toppling': 26007,\n",
              " 'topples': 26006,\n",
              " 'topping': 26005,\n",
              " 'topic': 26004,\n",
              " 'top-scorer': 26003,\n",
              " 'top-draw': 26002,\n",
              " 'tono': 26000,\n",
              " 'tongo': 25999,\n",
              " 'tones': 25998,\n",
              " 'tone': 25997,\n",
              " 'tonchetti': 25996,\n",
              " 'tomomi': 25995,\n",
              " 'tomich': 25994,\n",
              " 'tombe': 25993,\n",
              " 'tomasz': 25992,\n",
              " 'tolonics': 25990,\n",
              " 'toldo': 25985,\n",
              " 'tokyo-mitsubishi': 25984,\n",
              " 'toftir': 25982,\n",
              " 'toddler': 25979,\n",
              " 'tnt': 25977,\n",
              " 'tktx': 25975,\n",
              " 'tiwontschik': 25974,\n",
              " 'title-holders': 25972,\n",
              " 'titans': 25971,\n",
              " 'tissues': 25970,\n",
              " 'tiring': 25968,\n",
              " 'tire': 25967,\n",
              " 'tipping': 25964,\n",
              " 'tipper': 25962,\n",
              " 'unlit': 26278,\n",
              " 'tinsley': 25958,\n",
              " 'tino': 25957,\n",
              " 'timorese-born': 25954,\n",
              " 'timorese': 25953,\n",
              " 'timely': 25952,\n",
              " 'timeframe': 25951,\n",
              " 'timber': 25949,\n",
              " 'tillman': 25948,\n",
              " 'tilcon': 25946,\n",
              " 'tikhomirov-maskhadov': 25945,\n",
              " 'tier-one': 25941,\n",
              " 'tiebreak': 25940,\n",
              " 'tie-up': 25939,\n",
              " 'tie-breaker': 25938,\n",
              " 'tide': 25935,\n",
              " 'tibor': 25933,\n",
              " 'thyroid': 25929,\n",
              " 'thwarting': 25928,\n",
              " 'thursdays': 25926,\n",
              " 'thuram': 25925,\n",
              " 'thunder': 25923,\n",
              " 'thumped': 25922,\n",
              " 'thumbs': 25921,\n",
              " 'thumbnail': 25920,\n",
              " 'thul': 25919,\n",
              " 'thrifts': 25916,\n",
              " 'thrid': 25914,\n",
              " 'threesome': 25913,\n",
              " 'three-wicket': 25912,\n",
              " 'three-under': 25911,\n",
              " 'violently': 26455,\n",
              " 'three-session': 25908,\n",
              " 'three-putted': 25907,\n",
              " 'three-pointer': 25906,\n",
              " 'three-member': 25904,\n",
              " 'yam': 26750,\n",
              " 'three-': 25900,\n",
              " 'thoughts': 25899,\n",
              " 'thoughtless': 25898,\n",
              " 'thorsten': 25896,\n",
              " 'thorough': 25895,\n",
              " 'thomsen': 25893,\n",
              " 'thom': 25892,\n",
              " 'thmei': 25891,\n",
              " 'thiry': 25890,\n",
              " 'thirty': 25888,\n",
              " 'thirteen': 25887,\n",
              " 'thirst': 25885,\n",
              " 'third-year': 25883,\n",
              " 'third-stringer': 25882,\n",
              " 'third-seeded': 25881,\n",
              " 'third-quarter': 25880,\n",
              " 'third-placed': 25879,\n",
              " 'third-party': 25877,\n",
              " 'third-longest': 25876,\n",
              " 'third-busiest': 25875,\n",
              " 'thinly-veiled': 25874,\n",
              " 'thinakaran': 25873,\n",
              " 'thily': 25872,\n",
              " 'thessaloniki': 25871,\n",
              " 'therese': 25869,\n",
              " 'theory': 25868,\n",
              " 'theoretical': 25867,\n",
              " 'theodoros': 25865,\n",
              " 'theodore': 25864,\n",
              " 'then-u.s.': 25863,\n",
              " 'theatre-prop': 25862,\n",
              " 'thawra': 25861,\n",
              " 'thanking': 25860,\n",
              " 'thank': 25859,\n",
              " 'textile': 25855,\n",
              " 'texaco': 25853,\n",
              " 'testicular': 25849,\n",
              " 'tested-': 25848,\n",
              " 'territorial': 25845,\n",
              " 'terrified': 25844,\n",
              " 'terminates': 25842,\n",
              " 'terminated': 25841,\n",
              " 'tergat': 25840,\n",
              " 'tenth': 25838,\n",
              " 'tent': 25837,\n",
              " 'tenor': 25836,\n",
              " 'tennessee': 25835,\n",
              " 'tending': 25834,\n",
              " 'tendering': 25832,\n",
              " 'tendered': 25831,\n",
              " 'tendencies': 25830,\n",
              " 'tempest': 25827,\n",
              " 'tempelhof': 25825,\n",
              " 'tempe': 25824,\n",
              " 'tembau': 25823,\n",
              " 'telstra': 25822,\n",
              " 'telmex': 25820,\n",
              " 'tellez': 25819,\n",
              " 'televison': 25818,\n",
              " 'telekomunikasi': 25813,\n",
              " 'telecommunication': 25812,\n",
              " 'tekstilshik': 25809,\n",
              " 'tegucigalpa': 25806,\n",
              " 'tegel': 25804,\n",
              " 'teetotaller': 25803,\n",
              " 'teething': 25802,\n",
              " 'teeth': 25801,\n",
              " 'teemu': 25800,\n",
              " 'tecuari': 25797,\n",
              " 'toubon': 26020,\n",
              " 'technology-based': 25795,\n",
              " 'technologists': 25794,\n",
              " 'technological': 25793,\n",
              " 'technique': 25792,\n",
              " 'technically': 25790,\n",
              " 'technicalities': 25789,\n",
              " 'teamsystem': 25785,\n",
              " 'teamed': 25784,\n",
              " 'team-mates': 25783,\n",
              " 'teaching': 25782,\n",
              " 'tchmil': 25780,\n",
              " 'tbc': 25779,\n",
              " 'taxpayer': 25777,\n",
              " 'taxation': 25776,\n",
              " 'tax-raising': 25775,\n",
              " 'tax-free': 25774,\n",
              " 'tauranaga': 25771,\n",
              " 'tatiana': 25767,\n",
              " 'tatarella': 25764,\n",
              " 'tasmanian': 25762,\n",
              " 'tartous': 25761,\n",
              " 'tartabull': 25760,\n",
              " 'tarpon': 25759,\n",
              " 'tarnovo': 25758,\n",
              " 'tarnished': 25757,\n",
              " 'targest': 25756,\n",
              " 'tarawa': 25754,\n",
              " 'tarasov': 25753,\n",
              " 'tapes': 25751,\n",
              " 'tapani': 25750,\n",
              " 'tantrum': 25748,\n",
              " 'tantamount': 25747,\n",
              " 'tanny': 25746,\n",
              " 'tannery': 25745,\n",
              " 'tankage': 25744,\n",
              " 'tango': 25741,\n",
              " 'tando': 25740,\n",
              " 'tanaki': 25739,\n",
              " 'tana': 25738,\n",
              " 'unifed': 26260,\n",
              " 'tampico': 25737,\n",
              " 'tampere': 25736,\n",
              " 'tamil-speaking': 25734,\n",
              " 'tamer': 25733,\n",
              " 'tamaulipas': 25732,\n",
              " 'terroritsts': 25846,\n",
              " 'tally': 25729,\n",
              " 'tallahassee': 25727,\n",
              " 'talent-laden': 25724,\n",
              " 'talen': 25722,\n",
              " 'taleb': 25721,\n",
              " 'takings': 25720,\n",
              " 'taichung': 25712,\n",
              " 'kamiel': 21911,\n",
              " 'taiba': 25711,\n",
              " 'forsa': 11683,\n",
              " 'tactic': 25706,\n",
              " 'tackles': 25705,\n",
              " 'ta': 25701,\n",
              " '3.487': 16098,\n",
              " 't-shirt': 25698,\n",
              " 't-bonds': 25697,\n",
              " 'mets': 4415,\n",
              " 't-bills': 25696,\n",
              " 'syndicated': 25689,\n",
              " 'susana': 25635,\n",
              " 'syndicate': 25688,\n",
              " '1330': 4095,\n",
              " 'sympathisers': 25683,\n",
              " 'snow-scooter': 13460,\n",
              " 'symbols': 25679,\n",
              " 'tribute': 6192,\n",
              " 'symbolism': 25678,\n",
              " 'symbolic': 25676,\n",
              " 'sylvie': 25674,\n",
              " 'swore': 25670,\n",
              " 'reparations': 24443,\n",
              " 'sword': 25669,\n",
              " 'swooped': 25668,\n",
              " 'swim': 25661,\n",
              " 'swerford': 25658,\n",
              " 'technologies': 7528,\n",
              " 'swallow': 25649,\n",
              " 'svcd': 25646,\n",
              " 'suwandi': 25644,\n",
              " '+5.2': 14137,\n",
              " 'suspicions': 25640,\n",
              " 'jewell': 8773,\n",
              " 'suspend': 25639,\n",
              " '273': 7804,\n",
              " 'kit': 22023,\n",
              " '3565': 16258,\n",
              " 'susceptibility': 25636,\n",
              " 'description': 19739,\n",
              " 'survives': 25634,\n",
              " 'hire': 11950,\n",
              " 'survive': 25633,\n",
              " 'survival': 25632,\n",
              " 'quashed': 24166,\n",
              " 'surrogate': 25631,\n",
              " 'surin': 25626,\n",
              " 'surcin': 25624,\n",
              " 'oriente': 7226,\n",
              " 'suraj': 25623,\n",
              " 'supremo': 25622,\n",
              " '7:38.09': 17369,\n",
              " 'supremacy': 25621,\n",
              " 'suppose': 25619,\n",
              " 'supplying': 25617,\n",
              " 'shehu': 13364,\n",
              " '2-mth': 15534,\n",
              " 'supplied': 25616,\n",
              " 'urged': 3287,\n",
              " 'supervision': 25614,\n",
              " '97.38': 17651,\n",
              " 'supervised': 25613,\n",
              " '10-0-53-3': 14531,\n",
              " 'superstar': 25612,\n",
              " 'disagreement': 11350,\n",
              " '10-0-41-2': 14526,\n",
              " 'superiority': 25610,\n",
              " 'centers': 6622,\n",
              " 'sunstroke': 25606,\n",
              " 'sunny': 25604,\n",
              " '312-408-8721': 16175,\n",
              " 'sunk': 25603,\n",
              " '21/32': 10111,\n",
              " 'sung': 25600,\n",
              " 'sundstrom': 25599,\n",
              " 'gordana': 20925,\n",
              " 'summons': 25591,\n",
              " 'emotional': 5646,\n",
              " 'summoned': 25590,\n",
              " 'suite': 25584,\n",
              " 'suing': 25583,\n",
              " 'suddenly': 25573,\n",
              " 'sud-ptt': 25571,\n",
              " 'succession': 25566,\n",
              " 'sterling': 2751,\n",
              " 'substitutions': 25559,\n",
              " 'accuses': 3670,\n",
              " 'subsidy': 25558,\n",
              " '8863': 17512,\n",
              " 'subscription': 25556,\n",
              " 'subs': 25555,\n",
              " 'subandoro': 25548,\n",
              " 'sub': 25545,\n",
              " 'resolving': 13173,\n",
              " 'suan': 25544,\n",
              " 'dredging': 19987,\n",
              " 'gesture': 8575,\n",
              " 'stymies': 25543,\n",
              " '--': 60,\n",
              " 'styling': 25542,\n",
              " '5-170': 16744,\n",
              " 'stupidity': 25540,\n",
              " 'stupid': 25539,\n",
              " 'stumps': 25533,\n",
              " 'unauthorised': 13801,\n",
              " 'stumped': 25532,\n",
              " 'puk': 1332,\n",
              " 'stubbornly': 25527,\n",
              " 'strunz': 25525,\n",
              " 'strumming': 25524,\n",
              " 'stroud': 25522,\n",
              " 'strolled': 25517,\n",
              " 'stroked': 25516,\n",
              " '5-6': 16759,\n",
              " 'strips': 25512,\n",
              " 'stripes': 25507,\n",
              " \"shul'ala\": 25089,\n",
              " 'scorpion': 13307,\n",
              " 'robles': 24611,\n",
              " 'strides': 25505,\n",
              " 'stride': 25504,\n",
              " 'slaughter': 1609,\n",
              " 'stream': 25497,\n",
              " 'strawberry': 25495,\n",
              " 'strapped': 25493,\n",
              " '202.90': 15630,\n",
              " 'strapless': 25492,\n",
              " 'strange': 25489,\n",
              " 'straight-sets': 25485,\n",
              " 'glow': 20895,\n",
              " 'straddles': 25482,\n",
              " 'stonecutter': 25477,\n",
              " 'stockists': 25476,\n",
              " 'photo-fit': 23770,\n",
              " 'detonated': 19769,\n",
              " 'stock-trade': 25475,\n",
              " 'stock-swap': 25474,\n",
              " 'sweeping': 4040,\n",
              " 'stitches': 25472,\n",
              " 'stirring': 25471,\n",
              " 'athletic': 5472,\n",
              " 'stinnes': 25468,\n",
              " 'bombing': 2296,\n",
              " 'stinga': 25467,\n",
              " 'fail': 4882,\n",
              " 'stimulant': 25465,\n",
              " 'stilted': 25464,\n",
              " 'stig': 25461,\n",
              " 'stieglmair': 25459,\n",
              " 'sticking': 25457,\n",
              " 'steptoe': 25450,\n",
              " 'steppes': 25449,\n",
              " 'stephnopoulos': 25448,\n",
              " 'stephenson': 25447,\n",
              " 'stephanopoulos': 25446,\n",
              " 'single-family': 25144,\n",
              " 'stentex': 25444,\n",
              " 'hungary': 1320,\n",
              " '317': 6373,\n",
              " 'indre': 21562,\n",
              " 'stemming': 25443,\n",
              " 'amtrak': 5451,\n",
              " 'stem': 25441,\n",
              " 'steinberg': 25439,\n",
              " 'steinbach': 25438,\n",
              " 'stefaan': 25434,\n",
              " 'stud': 25529,\n",
              " '1897.1': 15210,\n",
              " 'steering': 25433,\n",
              " 'steels': 25431,\n",
              " 'serving': 2097,\n",
              " 'steel-producing': 25429,\n",
              " 'ruled': 1604,\n",
              " 'steamrollered': 25426,\n",
              " 'reaser': 13106,\n",
              " 'statute': 25420,\n",
              " 'state-controlled': 25416,\n",
              " 'bonilla': 5510,\n",
              " 'state-by-state': 25415,\n",
              " 'stas': 25411,\n",
              " 'wartime': 9682,\n",
              " 'staouelli': 25406,\n",
              " 'stanic': 25403,\n",
              " '32.': 16188,\n",
              " 'stances': 25402,\n",
              " 'stairs': 25394,\n",
              " '41-5/16': 16504,\n",
              " 'stabilising': 25389,\n",
              " 'srpska': 25385,\n",
              " 'cords': 11164,\n",
              " 'swine': 25662,\n",
              " 'squeezed': 25383,\n",
              " 'does': 649,\n",
              " 'stocher': 25473,\n",
              " 'sedan': 24909,\n",
              " '31/12': 16170,\n",
              " 'square-leg': 25381,\n",
              " '5/8': 5400,\n",
              " 'stefanel': 25435,\n",
              " 'square-jawed': 25380,\n",
              " 'spt': 25375,\n",
              " 'rowson': 24664,\n",
              " 'sprints': 25373,\n",
              " 'zanoli': 14040,\n",
              " 'sprinted': 25371,\n",
              " 'springs': 25369,\n",
              " 'spraying': 25367,\n",
              " 'sportsman': 25363,\n",
              " 'sporitelna': 25362,\n",
              " 'guelfs': 21015,\n",
              " 'spor': 25361,\n",
              " 'spooked': 25360,\n",
              " 'splits': 25353,\n",
              " 'split-up': 25352,\n",
              " 'spitting': 25349,\n",
              " 'spiral': 25345,\n",
              " 'stashing': 25412,\n",
              " 'spif': 25340,\n",
              " 'spectacle': 25332,\n",
              " 'deborah': 11261,\n",
              " 'specialised': 25327,\n",
              " 'spearheads': 25325,\n",
              " 'speakers': 25324,\n",
              " 'spasms': 25323,\n",
              " 'spares': 25321,\n",
              " 'spans': 25319,\n",
              " 'immigrants': 4353,\n",
              " 'spaniels': 25318,\n",
              " 'spal': 25317,\n",
              " 'spadea': 25316,\n",
              " 'spaces': 25315,\n",
              " 'sown': 25311,\n",
              " 'soviet-led': 25309,\n",
              " 'soveit-bloc': 25306,\n",
              " 'livestock': 3884,\n",
              " 'sousa': 25304,\n",
              " 'arabian': 18056,\n",
              " 'soured': 25303,\n",
              " 'doomed': 19948,\n",
              " 'pro-government': 13007,\n",
              " 'soundview': 25299,\n",
              " 'sorry': 25291,\n",
              " 'gunmen': 2885,\n",
              " 'sore': 25289,\n",
              " 'w': 657,\n",
              " 'sophomore': 25287,\n",
              " '174': 7750,\n",
              " 'averted': 18206,\n",
              " 'soothed': 25286,\n",
              " 'sonora': 25285,\n",
              " \"songo'o\": 25283,\n",
              " 'song-ho': 25282,\n",
              " 'solved': 25277,\n",
              " 'solskjaer': 25275,\n",
              " 'solovyov': 25274,\n",
              " 'solorz': 25273,\n",
              " 'sokoto': 25267,\n",
              " 'offal': 7214,\n",
              " 'transportation': 4052,\n",
              " 'respiratory': 24478,\n",
              " 'sokolovska': 25266,\n",
              " 'softs': 25264,\n",
              " 'sodigraf': 25261,\n",
              " 'galo': 11746,\n",
              " 'socha': 25253,\n",
              " 'sobral': 25252,\n",
              " 'sobotzik': 25251,\n",
              " 'flawed': 20578,\n",
              " ...}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "WORD_VOCAB.get_stoi()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "f5dc348d",
      "metadata": {
        "id": "f5dc348d"
      },
      "outputs": [],
      "source": [
        "# Get the word to idx and idx to word dictionaries\n",
        "wtoi = WORD_VOCAB.get_stoi()\n",
        "itow = WORD_VOCAB.get_itos()\n",
        "# Get the char to idx and idx to char dictionaries\n",
        "ctoi = CHAR_VOCAB.get_stoi()\n",
        "itoc = CHAR_VOCAB.get_itos()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2abb364f",
      "metadata": {
        "id": "2abb364f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "081c5ee8",
      "metadata": {
        "id": "081c5ee8"
      },
      "outputs": [],
      "source": [
        "assert(len(wtoi) == 26871)\n",
        "assert(len(ctoi) == 61)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5398c46",
      "metadata": {
        "id": "f5398c46"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "ac468388",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac468388",
        "outputId": "54cbaaad-f083-4fb9-ddfb-b95ad9b50c28"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0, 0)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# You should see 0 and 0 below\n",
        "WORD_VOCAB['<pad>'], CHAR_VOCAB['<pad>']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "d4227a0f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4227a0f",
        "outputId": "ed5ba3e7-50c5-4450-ec44-1171f16ba84d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, 1)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# You should see 1 and 1 below\n",
        "WORD_VOCAB['<unk>'], CHAR_VOCAB['<unk>']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "d81a25c9",
      "metadata": {
        "id": "d81a25c9"
      },
      "outputs": [],
      "source": [
        "# We need to carefully weight all the classes \n",
        "# We use w(c) = min(freq(l)) / freq(c); lower frequency classes \n",
        "# So a low class gets a weight that's higher, a higher class a lower weight\n",
        "# This function needs to return 3 dictionaries\n",
        "def get_label_id_map(labels):\n",
        "    # Get the unique list of labels\n",
        "    unique_labels = set([l for label in labels for l in label])\n",
        "    # Create a dictionary label to idx, starting with idx 0\n",
        "    ltoi = {value:index for index, value in enumerate(unique_labels)}\n",
        "    # Make a map from idx to label\n",
        "    itol = {index:value for index, value in enumerate(unique_labels)}\n",
        "    \n",
        "    itolw = {}\n",
        "    \n",
        "    label_to_count = {u_label:[l for label in labels for l in label].count(u_label) for u_label in unique_labels}\n",
        "    \n",
        "    for label, count in label_to_count.items():\n",
        "\n",
        "        itolw[ltoi[label]] = min(label_to_count.values())/count\n",
        "    \n",
        "    # Return (ltoi, itol, itolw)\n",
        "    return ltoi, itol, itolw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "546db172",
      "metadata": {
        "id": "546db172"
      },
      "outputs": [],
      "source": [
        "assert(len(pd.Series(chain(*train_labels)).unique()) == 9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "fa3b9fc5",
      "metadata": {
        "id": "fa3b9fc5"
      },
      "outputs": [],
      "source": [
        "ltoi, itol, itolw = get_label_id_map(train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "KMsQOeBSguWx",
      "metadata": {
        "id": "KMsQOeBSguWx"
      },
      "outputs": [],
      "source": [
        "for l, idx in ltoi.items():\n",
        "  assert(l == itol[idx])\n",
        "  assert(idx in itolw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "a9494010",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9494010",
        "outputId": "533fedd3-32b9-4559-e353-33a1f553f7d0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0: 0.175,\n",
              " 1: 0.25507950530035334,\n",
              " 2: 0.16176470588235295,\n",
              " 3: 1.0,\n",
              " 4: 0.18272425249169436,\n",
              " 5: 0.33595113438045376,\n",
              " 6: 0.31182505399568033,\n",
              " 7: 0.006811025015037328,\n",
              " 8: 0.9982713915298185}"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Look at the weights per tag\n",
        "itolw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "F5EtHfsQg7mB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5EtHfsQg7mB",
        "outputId": "42131c87-8ac0-40b3-f018-8c44eff9e9f9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'B-PER': 0,\n",
              " 'I-PER': 1,\n",
              " 'B-LOC': 2,\n",
              " 'I-MISC': 3,\n",
              " 'B-ORG': 4,\n",
              " 'B-MISC': 5,\n",
              " 'I-ORG': 6,\n",
              " 'O': 7,\n",
              " 'I-LOC': 8}"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ltoi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "BsJx4RzDRvk5",
      "metadata": {
        "id": "BsJx4RzDRvk5"
      },
      "outputs": [],
      "source": [
        "assert(min(itolw.values()) == 0.006811025015037328)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "96393bb8",
      "metadata": {
        "id": "96393bb8"
      },
      "outputs": [],
      "source": [
        "# Get the weights per class as a tensor of length 9; this will be needed in the loss to give different class elemets a different weight\n",
        "weights = torch.tensor([value for value in itolw.values()])\n",
        "for i, lw in itolw.items():\n",
        "    FILL_IN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "041bfde4",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "4b964cf9",
      "metadata": {
        "id": "4b964cf9"
      },
      "outputs": [],
      "source": [
        "# Set labels as a series\n",
        "labels = pd.Series([l for label in train_labels for l in label])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "r1hus59XV0Fy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1hus59XV0Fy",
        "outputId": "1b0780b2-8928-4b05-df58-aa19119a777d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0          B-ORG\n",
            "1              O\n",
            "2         B-MISC\n",
            "3              O\n",
            "4              O\n",
            "           ...  \n",
            "203616         O\n",
            "203617     B-ORG\n",
            "203618         O\n",
            "203619     B-ORG\n",
            "203620         O\n",
            "Length: 203621, dtype: object\n"
          ]
        }
      ],
      "source": [
        "print(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "80c8bf01",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80c8bf01",
        "outputId": "813e660c-1777-4891-ac70-7229b83d62a7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "O         169578\n",
              "B-LOC       7140\n",
              "B-PER       6600\n",
              "B-ORG       6321\n",
              "I-PER       4528\n",
              "I-ORG       3704\n",
              "B-MISC      3438\n",
              "I-LOC       1157\n",
              "I-MISC      1155\n",
              "dtype: int64"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get a count of labels and counts and print this below\n",
        "labels.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "8f2546c1",
      "metadata": {
        "id": "8f2546c1"
      },
      "outputs": [],
      "source": [
        "assert(labels.value_counts().min() == 1155)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebb5a61d",
      "metadata": {
        "id": "ebb5a61d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "bddf6405",
      "metadata": {
        "id": "bddf6405"
      },
      "source": [
        "### Check for class balance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "13522bf0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13522bf0",
        "outputId": "e13deb2c-8ca5-40cd-cb42-bf0a5a0816fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data label counts\n",
            "O         169578\n",
            "B-LOC       7140\n",
            "B-PER       6600\n",
            "B-ORG       6321\n",
            "I-PER       4528\n",
            "I-ORG       3704\n",
            "B-MISC      3438\n",
            "I-LOC       1157\n",
            "I-MISC      1155\n",
            "dtype: int64\n",
            "\n",
            "Validation data label counts\n",
            "O         42759\n",
            "B-PER      1842\n",
            "B-LOC      1837\n",
            "B-ORG      1341\n",
            "I-PER      1307\n",
            "B-MISC      922\n",
            "I-ORG       751\n",
            "I-MISC      346\n",
            "I-LOC       257\n",
            "dtype: int64\n",
            "\n",
            "Test data label counts\n",
            "O         38143\n",
            "B-ORG      1714\n",
            "B-LOC      1645\n",
            "B-PER      1617\n",
            "I-PER      1161\n",
            "I-ORG       881\n",
            "B-MISC      722\n",
            "I-LOC       259\n",
            "I-MISC      252\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Print the value count for each label\n",
        "print(\"Training data label counts\")\n",
        "print(pd.Series(chain(*train_labels)).value_counts())\n",
        "\n",
        "print(\"\\nValidation data label counts\")\n",
        "print(pd.Series(chain(*valid_labels)).value_counts())\n",
        "\n",
        "print(\"\\nTest data label counts\")\n",
        "print(pd.Series(chain(*test_labels)).value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "GVHS06FSWQs3",
      "metadata": {
        "id": "GVHS06FSWQs3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "ce51d1f7",
      "metadata": {
        "id": "ce51d1f7"
      },
      "source": [
        "### Series length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "eb6a72e7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb6a72e7",
        "outputId": "06285de4-093a-4a74-b23d-1b222da2eaab"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "count    14041.000000\n",
              "mean        14.501887\n",
              "std         11.602756\n",
              "min          1.000000\n",
              "25%          6.000000\n",
              "50%         10.000000\n",
              "75%         22.000000\n",
              "max        113.000000\n",
              "dtype: float64"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Display the mean sentence length for the training samples\n",
        "# You should get around 15 mean ...  What about median, 95%, etc?\n",
        "# .describe applied to a certain series is a good idea ...\n",
        "pd.Series([len(sentense.split(' ')) for sentense in train_sentences]).describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec9ef51e",
      "metadata": {
        "id": "ec9ef51e"
      },
      "source": [
        "### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "dc9dad21",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc9dad21",
        "outputId": "74111c83-7c3e-4626-ba46-b8514d362fbc"
      },
      "outputs": [],
      "source": [
        "# Size of token embeddings\n",
        "d_model = 300\n",
        "\n",
        "# Number of hidden units in the GRU layer\n",
        "d_hidden = 64\n",
        "\n",
        "# Number of hidden units in the GRU layer\n",
        "d_char = 32\n",
        "\n",
        "# Number of output nodes in the last layer\n",
        "num_classes = len(itol)\n",
        "\n",
        "# Number of samples in a batch\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "# Number of training epochs.\n",
        "EPOCHS = 25\n",
        "\n",
        "# FastText embeddings\n",
        "FAST_TEXT = FastText(\"simple\")\n",
        "\n",
        "# Learning rate\n",
        "LR = 1.0\n",
        "\n",
        "# Get the weights per class\n",
        "weight = weights\n",
        "\n",
        "# Maximum word length; critical for convolutions\n",
        "MAX_WORD_LENGTH = 12\n",
        "\n",
        "# The device to run on\n",
        "# Change this to 'mps' if you are on a mac with MPS\n",
        "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "380ba279",
      "metadata": {
        "id": "380ba279"
      },
      "outputs": [],
      "source": [
        "assert(len(train_sentences) // BATCH_SIZE == 109)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "23b33852",
      "metadata": {
        "id": "23b33852"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "7e6822d8",
      "metadata": {
        "id": "7e6822d8"
      },
      "outputs": [],
      "source": [
        "def collate_batch(batch):\n",
        "    label_list, sentence_list, sentence_lengths = [], [], []\n",
        "    word_list = []\n",
        "    # The sentence below is already transformed to int tokens\n",
        "    for sentence, words, labels in batch:\n",
        "        # Add the sentence to sentence_list list; you are added a tensor\n",
        "        sentence_list.append(torch.tensor(sentence))\n",
        "        # Add the sentence length to the right list\n",
        "        sentence_lengths.append(len(sentence))\n",
        "        # Add the labels to the right list\n",
        "        label_list.append(torch.tensor(labels))\n",
        "        # Add the words to the right list\n",
        "        word_list.append(torch.tensor(words))\n",
        "\n",
        "    # Return padded versions of the above; this function processes a batch remember so we need to return padded tensors\n",
        "    # batch_first=True below \n",
        "    # N = len(label_list)\n",
        "    # L_sentence = max(sentence_lengths)\n",
        "\n",
        "    return (\n",
        "        # (N, L_sentence) with the words \n",
        "        pad_sequence(sentence_list, batch_first=True, padding_value = WORD_VOCAB['<pad>']).to(DEVICE),\n",
        "        # (N, L_sentence) with the labels; set padding_val=-1 to ignore this in the loss\n",
        "        pad_sequence(label_list, batch_first=True, padding_value = -1).to(DEVICE),\n",
        "        FILL_IN,\n",
        "        # (N, L_sentence, L_word) where L_word (max) = 12\n",
        "        # This is padded at the word level, but not sentence level\n",
        "        pad_sequence(word_list, batch_first=True, padding_value = CHAR_VOCAB['<pad>']).to(DEVICE)\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "0a141dd7",
      "metadata": {
        "id": "0a141dd7"
      },
      "outputs": [],
      "source": [
        "def get_dl(sentences, labels):\n",
        "    # Maybe sort by the sentences by length so batches have roughly the same data?\n",
        "    \n",
        "    data = []\n",
        "    \n",
        "    # Note that we need to do our own \n",
        "    for sentence, labels in zip(sentences, labels):\n",
        "        word_tokens = WordTokenizer(sentence)\n",
        "        # Pass the word tokens through WORD_VOCAB\n",
        "        int_sentence = WORD_VOCAB(word_tokens)\n",
        "        int_words = []\n",
        "        for word_token in word_tokens:\n",
        "            # Append to word_token to int_words but tokenized; see below\n",
        "            int_words.append(\n",
        "                # Taking at most MAX_WORD_LENGTH tokens, get the list of tokens per character \n",
        "                # Note you need to add a list of variable '<pad>'s to make sure each element you add here has MAX_WORD_LENGTH\n",
        "                # You are adding to int_words a list of length MAX_WORD_LENGTH representing ints\n",
        "                # For example, if word_token = \"abc\", MAX_WORD_LENGTH = 5, this becomes \"abc<pad><pad>\" -> [1, 2, 3, 0, 0]\n",
        "                word_token\n",
        "            )\n",
        "        if len(int_words) > MAX_WORD_LENGTH:\n",
        "            int_words = int_words[:5]\n",
        "        else:\n",
        "            int_words.append(WORD_VOCAB['<pad>'] * (MAX_WORD_LENGTH - int_words))\n",
        "\n",
        "        # Create a list of int tokens for each label, use ltoi            \n",
        "        labels = [ltoi[label] for label in labels]\n",
        "        # You can remove these later\n",
        "        assert(len(int_sentence) == len(labels))\n",
        "        for int_word in int_words:\n",
        "          assert(len(int_word) == MAX_WORD_LENGTH)\n",
        "        data.append([int_sentence, int_words, labels])\n",
        "\n",
        "    # Return a DataLoader with batch_size=BATCH_SIZE, shuffle=True, and collate_fn=collate_batch    \n",
        "    return FILL_IN\n",
        "\n",
        "train_dl = get_dl(train_sentences, train_labels)\n",
        "valid_dl = get_dl(valid_sentences, valid_labels)\n",
        "test_dl = get_dl(test_sentences, test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "cfda3844",
      "metadata": {
        "id": "cfda3844"
      },
      "outputs": [],
      "source": [
        "assert(len(train_dl) == 110)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "8c0f1dad",
      "metadata": {
        "id": "8c0f1dad"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "9b308741",
      "metadata": {
        "id": "9b308741"
      },
      "outputs": [],
      "source": [
        "class GRUNERModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_class,\n",
        "        d_model, \n",
        "        d_hidden,\n",
        "        initialize = True,\n",
        "        fine_tune_embeddings = True,\n",
        "        use_conv_embeddings = True,\n",
        "    ):\n",
        "        \n",
        "        super(GRUNERModel, self).__init__()\n",
        "        self.vocab_size = len(WORD_VOCAB)\n",
        "        self.d_model = d_model\n",
        "        self.d_hidden = d_hidden\n",
        "        self.d_char = 32\n",
        "        self.kernel = 5\n",
        "        self.max_word_length = MAX_WORD_LENGTH\n",
        "        self.use_conv_embeddings = use_conv_embeddings\n",
        "        \n",
        "        if self.use_conv_embeddings:\n",
        "            # 12 - 5 + 1 = 8\n",
        "            # Input data will be (N * L_sentence, D_char, L_word)\n",
        "            # L_word = 12 here\n",
        "            # We want output to be d_char by 8 for self.kernel=5\n",
        "            self.conv = FILL_IN\n",
        "            # Will results in (N * L_sentence, D_char, 8) data.\n",
        "            # H_char is 32.\n",
        "            # Will result is (32, 1) vector for each word.\n",
        "            # Define a max pooling layer so the above holds\n",
        "            self.max_pool = FILL_IN\n",
        "    \n",
        "        # Create a word embedding layer with len(WORD_VOCAB) vectors; padding_idx=0 and set the length to 300 unless initialize=False in which case it is d_model\n",
        "        self.embedding = FILL_IN\n",
        "        \n",
        "        # Create a char embedding layer with len(CHAR_VOCAB) vectors; same as above but don't initialize with anything, make them d_char dimension\n",
        "        self.char_embedding = FILL_IN\n",
        "        \n",
        "        # Put in logic here to initialize the word embeddings or not with FAST_TEXT\n",
        "        # Make sure you map a word to its corrent word embedding in FAST_TEXT\n",
        "        if initialize:\n",
        "            FILL_IN\n",
        "        else:\n",
        "            self.init_weights()\n",
        "\n",
        "        # If fine_time_embeddings=False, turn off gradients for the word embeddings, they will be static\n",
        "        FILL_IN\n",
        "        \n",
        "        # Initialize a bidirectional GRU\n",
        "        # input is d_model + d_char (some other logic might be needed here if d_model != 300 given the above, but you can ignore this)\n",
        "        # Make batch_first=True; use self.d_hidden as the hidden dimension\n",
        "        self.rnn = FILL_IN\n",
        "\n",
        "        # Bidirectional GRU; so, we go from 2 * d_hidden to num_class via a linear layer\n",
        "        self.fc = FILL_IN\n",
        "\n",
        "        # Note: for drop out + ReLu, order does not matters\n",
        "        # Use 0.3 for the dropout probability\n",
        "        self.dropout = FILL_IN\n",
        "        \n",
        "    def init_weights(self):\n",
        "        # Initialize the word embedding layer with uniform random variables between (-initrange, initrange)\n",
        "        initrange = 0.5\n",
        "        # Add logic for the char embeddings also\n",
        "        FILL_IN\n",
        "\n",
        "    # N = batch_size,\n",
        "    # L_sentence = sequence length\n",
        "    # D_word = word embedding length\n",
        "    # D_char = char embedding length\n",
        "    # Hout = hidden dimenson from bidirectional GRU\n",
        "    # C = number of classes\n",
        "    def forward(self, sentences, lengths, words):\n",
        "        # (N, L_sentence, D_word)\n",
        "        embedded_sentences = self.embedding(sentences.int()) \n",
        "        \n",
        "        if self.use_conv_embeddings:                        \n",
        "            # (N, L_sentence, L_word, D_char)\n",
        "            # Pass words through the char_embeddings to get them\n",
        "            embedded_words = FILL_IN\n",
        "                                                \n",
        "            N, L_sentence, L_word, D_char = embedded_words.shape\n",
        "            \n",
        "            # (N * L_sentence, L_word, D_char)\n",
        "            # Reshape to the above dimension\n",
        "            embedded_words = FILL_IN\n",
        "\n",
        "            # (N * L_sentence, D_char, L_word)\n",
        "            # Do something to get the above dimension                        \n",
        "            embedded_words = FILL_IN\n",
        "                        \n",
        "            # 12 - 4, since kernel size is 5\n",
        "            # (N * L_sentence, D_char, L_word - kernel_size + 1 )\n",
        "            # Apply conv\n",
        "            embedded_words = FILL_IN\n",
        "                        \n",
        "            # (N * L_sentence, D_char, 1)\n",
        "            # Apply max pool and squeeze the result\n",
        "            embedded_words = FILL_IN\n",
        "                        \n",
        "            # (N, L_sentence, D_char)\n",
        "            # Reshape\n",
        "            embedded_words = FILL_IN\n",
        "\n",
        "            # (N, L_sentence, D_char + D_word)\n",
        "            # Concatenate a word's word vector and the character based word vector together\n",
        "            embedded_sentences = FILL_IN\n",
        "            \n",
        "        # This is a key for efficient computation. \n",
        "        # Pack the padded embeddings. Magic\n",
        "        embedded_sentences = FILL_IN\n",
        "        \n",
        "        # (N * L_sentence sort of, Hout)\n",
        "        logits, _ = FILL_IN\n",
        "        \n",
        "         # (N, L_sentence, Hout) \n",
        "         # Apply pad_packed_sequence to logits\n",
        "        logits, _ = FILL_IN\n",
        "\n",
        "        # (N, L_sentence, C)\n",
        "        # Apply self.fc\n",
        "        logits = FILL_IN\n",
        "        \n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "fb46b149",
      "metadata": {
        "id": "fb46b149"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "ccdb26b8",
      "metadata": {
        "id": "ccdb26b8"
      },
      "outputs": [],
      "source": [
        "# Used so we do not include padding indices.\n",
        "# Also, give different weights to different classes to account for class imbalance.\n",
        "# Use ignore_index=-1 since this is the \"pad\" index for labels\n",
        "criterion = FILL_IN\n",
        "\n",
        "# Define the model; use initialize=True, fine_tune=True, use_conv=True\n",
        "# I'm unsure if all these decisions are optimal, the point of this exercise is to make conv embeddings work\n",
        "model = FILL_IN\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "09270189",
      "metadata": {
        "id": "09270189"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "7754793c",
      "metadata": {
        "id": "7754793c"
      },
      "outputs": [],
      "source": [
        "from re import escape\n",
        "def train(dl, model, optimizer, criterion, epoch):\n",
        "    model.train()\n",
        "    total_acc, total_count = 0, 0\n",
        "    total_loss, total_batches = 0.0, 0.0\n",
        "    log_interval = 50\n",
        "\n",
        "    for idx, (sentences, labels, lengths, words) in enumerate(dl):\n",
        "        optimizer.zero_grad()\n",
        "                        \n",
        "        logits = model(sentences, lengths, words)\n",
        "                           \n",
        "        # Get the loss\n",
        "        N, L, _ = logits.shape\n",
        "        # Reshape to the right dimensons, and get the loss\n",
        "        logits = FILL_IN\n",
        "        labels = FILL_IN\n",
        "        loss = criterion(input=logits, target=labels)\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        total_batches += 1\n",
        "        \n",
        "        # Do back propagation\n",
        "        FILL_IN\n",
        "        \n",
        "        # Clip the gradients at 0.1\n",
        "        FILL_IN\n",
        "        \n",
        "        # Do an optimization step\n",
        "        FILL_IN\n",
        "\n",
        "        # Put in eval to get accuracies as below\n",
        "        FILL_IN\n",
        "\n",
        "        # Get the mask and then find out the predictions for things that are NOT masked\n",
        "        masks = FILL_IN\n",
        "        total_acc += FILL_IN\n",
        "        total_count += FILL_IN\n",
        "\n",
        "        model.train()\n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            print(\n",
        "                \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
        "                \"| accuracy {:8.3f} \"\n",
        "                \"| loss {:8.3f}\".format(\n",
        "                    epoch,\n",
        "                    idx,\n",
        "                    len(dl),\n",
        "                    total_acc / total_count,\n",
        "                    total_loss / total_batches\n",
        "                )\n",
        "            )\n",
        "            total_acc, total_count = 0, 0\n",
        "            total_loss, total_batches  = 0.0, 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "apQly1mOtfOe",
      "metadata": {
        "id": "apQly1mOtfOe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "e3cd7dc9",
      "metadata": {
        "id": "e3cd7dc9"
      },
      "outputs": [],
      "source": [
        "def evaluate(dl, model):\n",
        "    model.eval()\n",
        "    total_acc, total_count = 0, 0\n",
        "    total_loss, total_batches = 0.0, 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (sentences, labels, lengths, words) in enumerate(dl):\n",
        "            logits = model(sentences, lengths, words)\n",
        "            N, L, _ = logits.shape\n",
        "            # Very similar to train - reshape, get the accuracy for unmaked labels, etc\n",
        "            logits = FILL_IN\n",
        "            labels = FILL_IN\n",
        "            \n",
        "            total_loss += FILL_IN\n",
        "            total_batches += 1\n",
        "            \n",
        "            masks = FILL_IN\n",
        "            total_acc += FILL_IN\n",
        "            total_count += FILL_IN\n",
        "            \n",
        "    return total_acc / total_count, total_loss / total_batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "8b5d9836",
      "metadata": {
        "id": "8b5d9836"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "bb9e1f47",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb9e1f47",
        "outputId": "5f8c47d0-efff-41ba-d1c3-a347eb5d730c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| epoch   1 |    50/  110 batches | accuracy    0.483 | loss    1.935\n",
            "| epoch   1 |   100/  110 batches | accuracy    0.701 | loss    1.439\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   1 | time: 33.54s | valid accuracy    0.733 | valid loss    1.185 \n",
            "-----------------------------------------------------------\n",
            "| epoch   2 |    50/  110 batches | accuracy    0.754 | loss    1.156\n",
            "| epoch   2 |   100/  110 batches | accuracy    0.758 | loss    1.155\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   2 | time: 39.87s | valid accuracy    0.759 | valid loss    1.132 \n",
            "-----------------------------------------------------------\n",
            "| epoch   3 |    50/  110 batches | accuracy    0.753 | loss    1.124\n",
            "| epoch   3 |   100/  110 batches | accuracy    0.757 | loss    1.121\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   3 | time: 34.75s | valid accuracy    0.765 | valid loss    1.126 \n",
            "-----------------------------------------------------------\n",
            "| epoch   4 |    50/  110 batches | accuracy    0.755 | loss    1.130\n",
            "| epoch   4 |   100/  110 batches | accuracy    0.757 | loss    1.113\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   4 | time: 36.45s | valid accuracy    0.765 | valid loss    1.123 \n",
            "-----------------------------------------------------------\n",
            "| epoch   5 |    50/  110 batches | accuracy    0.756 | loss    1.117\n",
            "| epoch   5 |   100/  110 batches | accuracy    0.759 | loss    1.124\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   5 | time: 33.56s | valid accuracy    0.765 | valid loss    1.126 \n",
            "-----------------------------------------------------------\n",
            "| epoch   6 |    50/  110 batches | accuracy    0.759 | loss    1.109\n",
            "| epoch   6 |   100/  110 batches | accuracy    0.757 | loss    1.125\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   6 | time: 35.08s | valid accuracy    0.765 | valid loss    1.125 \n",
            "-----------------------------------------------------------\n",
            "| epoch   7 |    50/  110 batches | accuracy    0.758 | loss    1.121\n",
            "| epoch   7 |   100/  110 batches | accuracy    0.758 | loss    1.120\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   7 | time: 34.88s | valid accuracy    0.765 | valid loss    1.128 \n",
            "-----------------------------------------------------------\n",
            "| epoch   8 |    50/  110 batches | accuracy    0.757 | loss    1.119\n",
            "| epoch   8 |   100/  110 batches | accuracy    0.758 | loss    1.124\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   8 | time: 35.10s | valid accuracy    0.765 | valid loss    1.128 \n",
            "-----------------------------------------------------------\n",
            "| epoch   9 |    50/  110 batches | accuracy    0.756 | loss    1.127\n",
            "| epoch   9 |   100/  110 batches | accuracy    0.759 | loss    1.116\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   9 | time: 34.77s | valid accuracy    0.765 | valid loss    1.131 \n",
            "-----------------------------------------------------------\n",
            "| epoch  10 |    50/  110 batches | accuracy    0.759 | loss    1.125\n",
            "| epoch  10 |   100/  110 batches | accuracy    0.758 | loss    1.112\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  10 | time: 32.67s | valid accuracy    0.765 | valid loss    1.122 \n",
            "-----------------------------------------------------------\n",
            "| epoch  11 |    50/  110 batches | accuracy    0.759 | loss    1.123\n",
            "| epoch  11 |   100/  110 batches | accuracy    0.756 | loss    1.116\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  11 | time: 39.97s | valid accuracy    0.765 | valid loss    1.127 \n",
            "-----------------------------------------------------------\n",
            "| epoch  12 |    50/  110 batches | accuracy    0.759 | loss    1.120\n",
            "| epoch  12 |   100/  110 batches | accuracy    0.756 | loss    1.127\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  12 | time: 33.70s | valid accuracy    0.765 | valid loss    1.124 \n",
            "-----------------------------------------------------------\n",
            "| epoch  13 |    50/  110 batches | accuracy    0.757 | loss    1.120\n",
            "| epoch  13 |   100/  110 batches | accuracy    0.757 | loss    1.122\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  13 | time: 34.53s | valid accuracy    0.765 | valid loss    1.126 \n",
            "-----------------------------------------------------------\n",
            "| epoch  14 |    50/  110 batches | accuracy    0.759 | loss    1.111\n",
            "| epoch  14 |   100/  110 batches | accuracy    0.755 | loss    1.131\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  14 | time: 33.40s | valid accuracy    0.765 | valid loss    1.123 \n",
            "-----------------------------------------------------------\n",
            "| epoch  15 |    50/  110 batches | accuracy    0.758 | loss    1.127\n",
            "| epoch  15 |   100/  110 batches | accuracy    0.756 | loss    1.111\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  15 | time: 44.71s | valid accuracy    0.765 | valid loss    1.124 \n",
            "-----------------------------------------------------------\n",
            "| epoch  16 |    50/  110 batches | accuracy    0.757 | loss    1.123\n",
            "| epoch  16 |   100/  110 batches | accuracy    0.757 | loss    1.119\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  16 | time: 37.30s | valid accuracy    0.765 | valid loss    1.123 \n",
            "-----------------------------------------------------------\n",
            "| epoch  17 |    50/  110 batches | accuracy    0.759 | loss    1.119\n",
            "| epoch  17 |   100/  110 batches | accuracy    0.757 | loss    1.121\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  17 | time: 36.34s | valid accuracy    0.765 | valid loss    1.124 \n",
            "-----------------------------------------------------------\n",
            "| epoch  18 |    50/  110 batches | accuracy    0.758 | loss    1.120\n",
            "| epoch  18 |   100/  110 batches | accuracy    0.758 | loss    1.116\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  18 | time: 41.70s | valid accuracy    0.765 | valid loss    1.126 \n",
            "-----------------------------------------------------------\n",
            "| epoch  19 |    50/  110 batches | accuracy    0.759 | loss    1.105\n",
            "| epoch  19 |   100/  110 batches | accuracy    0.757 | loss    1.143\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  19 | time: 40.58s | valid accuracy    0.765 | valid loss    1.127 \n",
            "-----------------------------------------------------------\n",
            "| epoch  20 |    50/  110 batches | accuracy    0.757 | loss    1.127\n",
            "| epoch  20 |   100/  110 batches | accuracy    0.758 | loss    1.112\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  20 | time: 40.17s | valid accuracy    0.765 | valid loss    1.124 \n",
            "-----------------------------------------------------------\n",
            "| epoch  21 |    50/  110 batches | accuracy    0.758 | loss    1.130\n",
            "| epoch  21 |   100/  110 batches | accuracy    0.757 | loss    1.103\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  21 | time: 40.64s | valid accuracy    0.765 | valid loss    1.130 \n",
            "-----------------------------------------------------------\n",
            "| epoch  22 |    50/  110 batches | accuracy    0.758 | loss    1.116\n",
            "| epoch  22 |   100/  110 batches | accuracy    0.758 | loss    1.121\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  22 | time: 38.26s | valid accuracy    0.765 | valid loss    1.127 \n",
            "-----------------------------------------------------------\n",
            "| epoch  23 |    50/  110 batches | accuracy    0.758 | loss    1.123\n",
            "| epoch  23 |   100/  110 batches | accuracy    0.758 | loss    1.126\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  23 | time: 42.33s | valid accuracy    0.765 | valid loss    1.121 \n",
            "-----------------------------------------------------------\n",
            "| epoch  24 |    50/  110 batches | accuracy    0.760 | loss    1.115\n",
            "| epoch  24 |   100/  110 batches | accuracy    0.756 | loss    1.127\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  24 | time: 43.65s | valid accuracy    0.765 | valid loss    1.128 \n",
            "-----------------------------------------------------------\n",
            "| epoch  25 |    50/  110 batches | accuracy    0.758 | loss    1.123\n",
            "| epoch  25 |   100/  110 batches | accuracy    0.758 | loss    1.119\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  25 | time: 40.53s | valid accuracy    0.765 | valid loss    1.123 \n",
            "-----------------------------------------------------------\n",
            "Checking the results of test dataset.\n",
            "test accuracy    0.754 | test loss    1.133\n"
          ]
        }
      ],
      "source": [
        "from time import time\n",
        "import time\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(train_dl, model, optimizer, criterion, epoch)\n",
        "    accu_val, loss_val = evaluate(valid_dl, model)\n",
        "    scheduler.step()\n",
        "    print(\"-\" * 59)\n",
        "    print(\n",
        "        \"| end of epoch {:3d} | time: {:5.2f}s \"\n",
        "        \"| valid accuracy {:8.3f} \"\n",
        "        \"| valid loss {:8.3f} \".format(\n",
        "            epoch,\n",
        "            time.time() - epoch_start_time,\n",
        "            accu_val,\n",
        "            loss_val\n",
        "        )\n",
        "    )\n",
        "    print(\"-\" * 59)\n",
        "\n",
        "print(\"Checking the results of test dataset.\")\n",
        "accu_test, loss_test = evaluate(test_dl, model)\n",
        "print(\"test accuracy {:8.3f} | test loss {:8.3f}\".format(accu_test, loss_test))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.8 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "16ed27859114ec3a2ea86330879d9bf5297ca3d5c6dfd17134827f43bf0b03b0"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
